{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training ImageNet TFRecords with Nvidia DALI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import socket\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils import data\n",
    "import torchvision.models as models\n",
    "\n",
    "from torchinfo import summary\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loader': 2502, 'valid_loader': 125}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dali_pt_dataloader import dali_dataloader\n",
    "\n",
    "num_shards = 2 # world size\n",
    "shard_id = 0   # rank\n",
    "\n",
    "train_loader = dali_dataloader(\n",
    "    batch_size=256,\n",
    "    num_threads=os.cpu_count(),\n",
    "    tfrec_filenames=sorted(glob('/scratch/snx3000/datasets/imagenet/ILSVRC2012_1k/train/*')),\n",
    "    tfrec_idx_filenames=sorted(glob('/scratch/snx3000/datasets/imagenet/ILSVRC2012_1k/idx_files/train/*')),\n",
    "    shard_id=shard_id,\n",
    "    num_shards=num_shards,\n",
    "    gpu_aug=True,\n",
    "    gpu_out=True,\n",
    "    training=True,\n",
    ")\n",
    "valid_loader = dali_dataloader(\n",
    "    batch_size=200,\n",
    "    num_threads=os.cpu_count(),\n",
    "    tfrec_filenames=sorted(glob('/scratch/snx3000/datasets/imagenet/ILSVRC2012_1k/validation/*')),\n",
    "    tfrec_idx_filenames=sorted(glob('/scratch/snx3000/datasets/imagenet/ILSVRC2012_1k/idx_files/validation/*')),\n",
    "    shard_id=shard_id,\n",
    "    num_shards=num_shards,\n",
    "    gpu_aug=True,\n",
    "    gpu_out=True,\n",
    "    training=False,\n",
    ")\n",
    "\n",
    "dict(train_loader=len(train_loader), valid_loader=len(valid_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential                               --                        --\n",
       "├─ConvBlock: 1-1                         [8, 32, 112, 112]         --\n",
       "│    └─Conv2d: 2-1                       [8, 32, 112, 112]         1,536\n",
       "│    └─BatchNorm2d: 2-2                  [8, 32, 112, 112]         64\n",
       "│    └─SiLU: 2-3                         [8, 32, 112, 112]         --\n",
       "├─ConvBlock: 1-2                         [8, 64, 56, 56]           --\n",
       "│    └─Conv2d: 2-4                       [8, 64, 56, 56]           32,768\n",
       "│    └─BatchNorm2d: 2-5                  [8, 64, 56, 56]           128\n",
       "│    └─SiLU: 2-6                         [8, 64, 56, 56]           --\n",
       "├─ConvBlock: 1-3                         [8, 128, 28, 28]          --\n",
       "│    └─Conv2d: 2-7                       [8, 128, 28, 28]          131,072\n",
       "│    └─BatchNorm2d: 2-8                  [8, 128, 28, 28]          256\n",
       "│    └─SiLU: 2-9                         [8, 128, 28, 28]          --\n",
       "├─ConvBlock: 1-4                         [8, 256, 14, 14]          --\n",
       "│    └─Conv2d: 2-10                      [8, 256, 14, 14]          524,288\n",
       "│    └─BatchNorm2d: 2-11                 [8, 256, 14, 14]          512\n",
       "│    └─SiLU: 2-12                        [8, 256, 14, 14]          --\n",
       "├─ConvBlock: 1-5                         [8, 512, 7, 7]            --\n",
       "│    └─Conv2d: 2-13                      [8, 512, 7, 7]            2,097,152\n",
       "│    └─BatchNorm2d: 2-14                 [8, 512, 7, 7]            1,024\n",
       "│    └─SiLU: 2-15                        [8, 512, 7, 7]            --\n",
       "├─ConvBlock: 1-6                         [8, 1024, 7, 7]           --\n",
       "│    └─Conv2d: 2-16                      [8, 1024, 7, 7]           4,718,592\n",
       "│    └─BatchNorm2d: 2-17                 [8, 1024, 7, 7]           2,048\n",
       "│    └─SiLU: 2-18                        [8, 1024, 7, 7]           --\n",
       "├─AdaptiveAvgPool2d: 1-7                 [8, 1024, 1, 1]           --\n",
       "├─Flatten: 1-8                           [8, 1024]                 --\n",
       "├─Dropout: 1-9                           [8, 1024]                 --\n",
       "├─Linear: 1-10                           [8, 1000]                 1,024,000\n",
       "==========================================================================================\n",
       "Total params: 8,533,440\n",
       "Trainable params: 8,533,440\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 5.30\n",
       "==========================================================================================\n",
       "Input size (MB): 4.82\n",
       "Forward/backward pass size (MB): 106.04\n",
       "Params size (MB): 34.13\n",
       "Estimated Total Size (MB): 144.99\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = 1000\n",
    "img_size = 224\n",
    "\n",
    "class ConvBlock(nn.Sequential):\n",
    "    def __init__(self, channels_in, channels_out, kernel_size=4, padding=1, stride=2, bias=False):\n",
    "        super().__init__(OrderedDict([\n",
    "            ('conv', nn.Conv2d(channels_in, channels_out, kernel_size=kernel_size, padding=padding, stride=stride, bias=bias)),\n",
    "            ('bn', nn.BatchNorm2d(channels_out)),\n",
    "            ('act', nn.SiLU()),\n",
    "        ]))\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('b224', ConvBlock(3, 32)),\n",
    "    ('b112', ConvBlock(32, 64)),\n",
    "    ('b56', ConvBlock(64, 128)),\n",
    "    ('b28', ConvBlock(128, 256)),\n",
    "    ('b14', ConvBlock(256, 512)),\n",
    "    ('b7', ConvBlock(512, 1024, kernel_size=3, padding=1, stride=1)),\n",
    "    ('avg', nn.AdaptiveAvgPool2d(1)),\n",
    "    ('flat', nn.Flatten()),\n",
    "    ('drop', nn.Dropout(0.2)),\n",
    "    ('classifier', nn.Linear(1024, n_classes, bias=False)),\n",
    "]))\n",
    "\n",
    "summary(model, input_size=(8, 3, img_size, img_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_classifier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_classifier.py\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import trange, tqdm\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, scheduler, logger, log_every=10):\n",
    "    losses = RunningAverage('Loss')\n",
    "    acc = RunningAverage('Acc')\n",
    "    model.train()\n",
    "    \n",
    "    for i, (batch,) in enumerate(tqdm(train_loader, leave=False)):\n",
    "        imgs, target = batch['data'], batch['label'].ravel()\n",
    "\n",
    "        # Calculate CE loss\n",
    "        logits = F.log_softmax(model(imgs), dim=1)\n",
    "        loss = F.nll_loss(logits, target)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Calculate gradients\n",
    "        loss.backward()\n",
    "        # Training step\n",
    "        optimizer.step()\n",
    "        # Update LR\n",
    "        scheduler.step()\n",
    "\n",
    "        # Update metrics\n",
    "        with torch.no_grad():\n",
    "            losses.update(loss.item())\n",
    "            acc.update((logits.argmax(dim=1) == target).float().mean().item())\n",
    "            if i % log_every == 0:\n",
    "                logger(f'{acc} {losses} lr:{optimizer.param_groups[0][\"lr\"]:.01e}')\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, valid_loader):\n",
    "    losses = AverageMeter('Loss')\n",
    "    acc = AverageMeter('Acc')\n",
    "    model.eval()\n",
    "\n",
    "    for i, (batch,) in enumerate(tqdm(valid_loader, leave=False)):\n",
    "        imgs, target = batch['data'], batch['label'].ravel()\n",
    "        # Calculate CE loss\n",
    "        logits = F.log_softmax(model(imgs), dim=1)\n",
    "        loss = F.nll_loss(logits, target)\n",
    "        # Update metrics\n",
    "        losses.update(loss.item(), len(target))\n",
    "        acc.update((logits.argmax(dim=1) == target).float().mean().item(), len(target))\n",
    "\n",
    "    return losses, acc\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':.03f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def all_reduce(self, device='cuda'):\n",
    "        total = torch.FloatTensor([self.sum, self.count], device=device)\n",
    "        dist.all_reduce(total, dist.ReduceOp.SUM)\n",
    "        self.sum, self.count = total.tolist()\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name}:{avg' + self.fmt + '}'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "    \n",
    "class RunningAverage(object):\n",
    "    \"\"\"Computes and stores the running average of the given value\"\"\"\n",
    "    def __init__(self, name, fmt=':.03f', beta=0.98):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.beta = beta\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg = None\n",
    "\n",
    "    def update(self, val):\n",
    "        if self.avg is None:\n",
    "            self.avg = val\n",
    "        self.avg = self.beta*self.avg + (1-self.beta)*val\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name}:{avg' + self.fmt + '}'\n",
    "        return fmtstr.format(**self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc422226e41642f0b622167abcea741c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2502 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Acc:0.070 Loss:5.711\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f0361d20634a4dbfff2b527a7d5bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2502 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from train_classifier import train, validate\n",
    "\n",
    "epochs = 2\n",
    "history = []\n",
    "lr = 0.05\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr, momentum=0.9, weight_decay=1e-5, nesterov=True)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
    "                                                max_lr=lr,\n",
    "                                                epochs=epochs,\n",
    "                                                steps_per_epoch=len(train_loader),\n",
    "                                                pct_start=0.1,\n",
    "                                                final_div_factor=100)\n",
    "\n",
    "pbar = trange(epochs)\n",
    "for e in pbar:\n",
    "    train(model, train_loader, optimizer, scheduler, logger=pbar.set_description)\n",
    "    losses, acc = validate(model, valid_loader)\n",
    "    print(f'{e}: {acc} {losses}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<mark>Exercise</mark>:\n",
    "\n",
    "1. Use ipcmagic and DDP to distribute the ImageNet training\n",
    "   - shard also the validation set and make sure the reported loss and accuracy are averaged between the nodes\n",
    "2. Create a Python script with the same code and launch it using `sbatch`\n",
    "   - e.g. `sbatch train_imagenet.sh` _(If you are not familiar with running jobs on Piz Daint, please read the [CSCS user page](https://user.cscs.ch/access/running/), additional details to [Piz Daint](https://user.cscs.ch/access/running/piz_daint/), and other examples with [PyTorch](https://user.cscs.ch/computing/data_science/pytorch/))_\n",
    "   - Try different hyperparameters, schedulers, optimizers, etc. For that, it might be useful to handle parameters with [argparse](https://docs.python.org/3/library/argparse.html) or another similar module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import socket\n",
    "# import ipcmagic\n",
    "# %ipcluster start -n 2 --mpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %ipcluster stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepspeed",
   "language": "python",
   "name": "deepspeed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
