{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding ImageNet TFRecords with Nvidia DALI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import socket\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils import data\n",
    "import torchvision.models as models\n",
    "\n",
    "from torchsummary import summary\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dali_pt_dataloader import dali_dataloader\n",
    "\n",
    "train_loader = dali_dataloader(\n",
    "    batch_size=256,\n",
    "    num_threads=os.cpu_count(),\n",
    "    tfrec_filenames=sorted(glob('/scratch/snx3000/datasets/imagenet/ILSVRC2012_1k/train/*')),\n",
    "    tfrec_idx_filenames=sorted(glob('/scratch/snx3000/datasets/imagenet/ILSVRC2012_1k/idx_files/train/*')),\n",
    "    shard_id=0,\n",
    "    num_shards=1,\n",
    "    gpu_aug=True,\n",
    "    gpu_out=True,\n",
    "    training=True,\n",
    ")\n",
    "valid_loader = dali_dataloader(\n",
    "    batch_size=200,\n",
    "    num_threads=os.cpu_count(),\n",
    "    tfrec_filenames=sorted(glob('/scratch/snx3000/datasets/imagenet/ILSVRC2012_1k/validation/*')),\n",
    "    tfrec_idx_filenames=sorted(glob('/scratch/snx3000/datasets/imagenet/ILSVRC2012_1k/idx_files/validation/*')),\n",
    "    shard_id=0,\n",
    "    num_shards=1,\n",
    "    gpu_aug=True,\n",
    "    gpu_out=True,\n",
    "    training=False,\n",
    ")\n",
    "\n",
    "dict(train_loader=len(train_loader), valid_loader=len(valid_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 1000\n",
    "img_size = 224\n",
    "\n",
    "class ConvBlock(nn.Sequential):\n",
    "    def __init__(self, channels_in, channels_out, kernel_size=4, padding=1, stride=2, bias=False):\n",
    "        super().__init__(OrderedDict([\n",
    "            ('conv', nn.Conv2d(channels_in, channels_out, kernel_size=kernel_size, padding=padding, stride=stride, bias=bias)),\n",
    "            ('bn', nn.BatchNorm2d(channels_out)),\n",
    "            ('act', nn.SiLU()),\n",
    "        ]))\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('b224', ConvBlock(3, 32)),\n",
    "    ('b112', ConvBlock(32, 64)),\n",
    "    ('b56', ConvBlock(64, 128)),\n",
    "    ('b28', ConvBlock(128, 256)),\n",
    "    ('b14', ConvBlock(256, 512)),\n",
    "    ('b7', ConvBlock(512, 1024, kernel_size=3, padding=1, stride=1)),\n",
    "    ('avg', nn.AdaptiveAvgPool2d(1)),\n",
    "    ('flat', nn.Flatten()),\n",
    "    ('drop', nn.Dropout(0.2)),\n",
    "    ('classifier', nn.Linear(1024, n_classes, bias=False)),\n",
    "]))\n",
    "\n",
    "summary(model, (3, img_size, img_size), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_classifier.py\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import trange, tqdm\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, scheduler, logger, log_every=10):\n",
    "    losses = RunningAverage('Loss')\n",
    "    acc = RunningAverage('Acc')\n",
    "    model.train()\n",
    "    \n",
    "    for i, (batch,) in enumerate(tqdm(train_loader, leave=False)):\n",
    "        imgs, target = batch['data'], batch['label'].ravel()\n",
    "\n",
    "        # Calculate CE loss\n",
    "        logits = F.log_softmax(model(imgs), dim=1)\n",
    "        loss = F.nll_loss(logits, target)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Calculate gradients\n",
    "        loss.backward()\n",
    "        # Training step\n",
    "        optimizer.step()\n",
    "        # Update LR\n",
    "        scheduler.step()\n",
    "\n",
    "        # Update metrics\n",
    "        with torch.no_grad():\n",
    "            losses.update(loss.item())\n",
    "            acc.update((logits.argmax(dim=1) == target).float().mean().item())\n",
    "            if i % log_every == 0:\n",
    "                logger(f'{acc} {losses} lr:{optimizer.param_groups[0][\"lr\"]:.01e}')\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, valid_loader):\n",
    "    losses = AverageMeter('Loss')\n",
    "    acc = AverageMeter('Acc')\n",
    "    model.eval()\n",
    "\n",
    "    for i, (batch,) in enumerate(tqdm(valid_loader, leave=False)):\n",
    "        imgs, target = batch['data'], batch['label'].ravel()\n",
    "        # Calculate CE loss\n",
    "        logits = F.log_softmax(model(imgs), dim=1)\n",
    "        loss = F.nll_loss(logits, target)\n",
    "        # Update metrics\n",
    "        losses.update(loss.item(), len(target))\n",
    "        acc.update((logits.argmax(dim=1) == target).float().mean().item(), len(target))\n",
    "\n",
    "    return losses, acc\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':.03f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def all_reduce(self, device='cuda'):\n",
    "        total = torch.FloatTensor([self.sum, self.count], device=device)\n",
    "        dist.all_reduce(total, dist.ReduceOp.SUM)\n",
    "        self.sum, self.count = total.tolist()\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name}:{avg' + self.fmt + '}'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "    \n",
    "class RunningAverage(object):\n",
    "    \"\"\"Computes and stores the running average of the given value\"\"\"\n",
    "    def __init__(self, name, fmt=':.03f', beta=0.98):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.beta = beta\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg = None\n",
    "\n",
    "    def update(self, val):\n",
    "        if self.avg is None:\n",
    "            self.avg = val\n",
    "        self.avg = self.beta*self.avg + (1-self.beta)*val\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name}:{avg' + self.fmt + '}'\n",
    "        return fmtstr.format(**self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_classifier import train, validate\n",
    "\n",
    "epochs = 10\n",
    "history = []\n",
    "lr = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr, momentum=0.9, weight_decay=1e-5, nesterov=True)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
    "                                                max_lr=lr,\n",
    "                                                epochs=epochs,\n",
    "                                                steps_per_epoch=len(train_loader),\n",
    "                                                pct_start=0.01,\n",
    "                                                final_div_factor=100)\n",
    "\n",
    "pbar = trange(epochs)\n",
    "for e in pbar:\n",
    "    train(model, train_loader, optimizer, scheduler, logger=pbar.set_description)\n",
    "    losses, acc = validate(model, valid_loader)\n",
    "    print(f'{e}: {acc} {losses}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
