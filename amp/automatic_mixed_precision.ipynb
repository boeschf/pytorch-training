{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch AMP\n",
    "\n",
    "Automatic Mixed Precision Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Float precision\n",
    "\n",
    "<table class=\"wikitable\" style=\"text-align:center; border-width:0; margin-left: 0 !important;\">\n",
    "<caption><a href=\"https://en.wikipedia.org/wiki/Single-precision_floating-point_format\" title=\"Single-precision floating-point format\">IEEE 754 single-precision</a> 32-bit float\n",
    "</caption>\n",
    "<tbody><tr style=\"line-height:70%;\">\n",
    "<td style=\"border-width:0; background:#FFFFFF;\" colspan=\"2\">sign\n",
    "</td>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\" colspan=\"8\">exponent (8 bit)\n",
    "</td>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\" colspan=\"23\">fraction (23 bit)\n",
    "</td></tr>\n",
    "<tr style=\"line-height:70%;\">\n",
    "<td style=\"border-width:0; background:#FFFFFF;\" colspan=\"2\">&nbsp;&nbsp;┃\n",
    "</td>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\" colspan=\"8\">┌─────────────┐\n",
    "</td>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\" colspan=\"23\">┌───────────────────────────────────────────┐\n",
    "</td></tr>\n",
    "<tr style=\"font-size:9pt;\">\n",
    "<td style=\"border-width:0; background:#FFFFFF;\"></td>\n",
    "<td style=\"background:#C4FCFF;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#9FFFAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#9FFFAC;\">&nbsp;1&nbsp;</td>\n",
    "<td style=\"background:#9FFFAC;\">&nbsp;1&nbsp;</td>\n",
    "<td style=\"background:#9FFFAC;\">&nbsp;1&nbsp;</td>\n",
    "<td style=\"background:#9FFFAC;\">&nbsp;1&nbsp;</td>\n",
    "<td style=\"background:#9FFFAC;\">&nbsp;1&nbsp;</td>\n",
    "<td style=\"background:#9FFFAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#9FFFAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;1&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;\n",
    "</td></tr>\n",
    "<tr>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\"></td>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\">31</td>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\">30</td>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\" colspan=\"6\"></td>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\">23</td>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\">22</td>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\" colspan=\"21\"></td>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\">0\n",
    "</td></tr></tbody></table>\n",
    "\n",
    "<table class=\"wikitable\" style=\"text-align:center; border-width:0; margin-left: 0 !important;\">\n",
    "<caption><a href=\"https://en.wikipedia.org/wiki/Half-precision_floating-point_format\" title=\"Half-precision floating-point format\">IEEE half-precision</a> 16-bit float\n",
    "</caption>\n",
    "<tbody><tr style=\"line-height:70%;\">\n",
    "<td style=\"border-width:0; background:#FFFFFF;\" colspan=\"2\">sign\n",
    "</td>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\" colspan=\"5\">exponent (5 bit)\n",
    "</td>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\" colspan=\"10\">fraction (10 bit)\n",
    "</td></tr>\n",
    "<tr style=\"line-height:70%;\">\n",
    "<td style=\"border-width:0; background:#FFFFFF;\" colspan=\"2\">&nbsp;&nbsp;┃\n",
    "</td>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\" colspan=\"5\">┌───────┐\n",
    "</td>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\" colspan=\"10\">┌─────────────────┐\n",
    "</td></tr>\n",
    "<tr style=\"font-size:9pt;\">\n",
    "<td style=\"border-width:0; background:#FFFFFF;\"></td>\n",
    "<td style=\"background:#C4FCFF;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#9FFFAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#9FFFAC;\">&nbsp;1&nbsp;</td>\n",
    "<td style=\"background:#9FFFAC;\">&nbsp;1&nbsp;</td>\n",
    "<td style=\"background:#9FFFAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#9FFFAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;1&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;\n",
    "</td></tr>\n",
    "<tr>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\"></td>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\">15</td>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\">14</td>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\" colspan=\"3\"></td>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\">10</td>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\">9</td>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\" colspan=\"8\"></td>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\">0\n",
    "</td></tr></tbody></table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import gc\n",
    "  \n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import apex\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils import data\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import torch.utils.benchmark as benchmark\n",
    "\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine parameters for float32\n",
      "---------------------------------------------------------------\n",
      "precision =   6   resolution = 1.0000000e-06\n",
      "machep =    -23   eps =        1.1920929e-07\n",
      "negep =     -24   epsneg =     5.9604645e-08\n",
      "minexp =   -126   tiny =       1.1754944e-38\n",
      "maxexp =    128   max =        3.4028235e+38\n",
      "nexp =        8   min =        -max\n",
      "---------------------------------------------------------------\n",
      "\n",
      "Machine parameters for float16\n",
      "---------------------------------------------------------------\n",
      "precision =   3   resolution = 1.00040e-03\n",
      "machep =    -10   eps =        9.76562e-04\n",
      "negep =     -11   epsneg =     4.88281e-04\n",
      "minexp =    -14   tiny =       6.10352e-05\n",
      "maxexp =     16   max =        6.55040e+04\n",
      "nexp =        5   min =        -max\n",
      "---------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(np.finfo(np.float32))\n",
    "print(np.finfo(np.float16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "\n",
    "<table class=\"wikitable\" style=\"text-align:center; border-width:0; margin-left: 0 !important;\">\n",
    "<caption><a href=\"https://en.wikipedia.org/wiki/Bfloat16_floating-point_format\" title=\"Half-precision floating-point format\">Google Brain's bfloat16</a>\n",
    "(limited hardware support)</caption>\n",
    "<tbody><tr style=\"line-height:70%;\">\n",
    "<td style=\"border-width:0; background:#FFFFFF;\" colspan=\"2\">sign\n",
    "</td>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\" colspan=\"8\">exponent (8 bit)\n",
    "</td>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\" colspan=\"7\">fraction (7 bit)\n",
    "</td></tr>\n",
    "<tr style=\"line-height:70%;\">\n",
    "<td style=\"border-width:0; background:#FFFFFF;\" colspan=\"2\">&nbsp;&nbsp;┃\n",
    "</td>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\" colspan=\"8\">┌─────────────┐\n",
    "</td>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\" colspan=\"7\">┌───────────┐\n",
    "</td></tr>\n",
    "<tr style=\"font-size:9pt;\">\n",
    "<td style=\"border-width:0; background:#FFFFFF;\"></td>\n",
    "<td style=\"background:#C4FCFF;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#9FFFAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#9FFFAC;\">&nbsp;1&nbsp;</td>\n",
    "<td style=\"background:#9FFFAC;\">&nbsp;1&nbsp;</td>\n",
    "<td style=\"background:#9FFFAC;\">&nbsp;1&nbsp;</td>\n",
    "<td style=\"background:#9FFFAC;\">&nbsp;1&nbsp;</td>\n",
    "<td style=\"background:#9FFFAC;\">&nbsp;1&nbsp;</td>\n",
    "<td style=\"background:#9FFFAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#9FFFAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;1&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;</td>\n",
    "<td style=\"background:#FFACAC;\">&nbsp;0&nbsp;\n",
    "</td></tr>\n",
    "<tr>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\"></td>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\">15</td>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\">14</td>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\" colspan=\"6\"></td>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\">7</td>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\">6</td>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\" colspan=\"5\"></td>\n",
    "<td style=\"border-width:0; background:#FFFFFF;\">0\n",
    "</td></tr></tbody></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finfo(resolution=0.01, min=-3.38953e+38, max=3.38953e+38, eps=0.0078125, tiny=1.17549e-38, dtype=bfloat16)\n"
     ]
    }
   ],
   "source": [
    "print(torch.finfo(torch.bfloat16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0002e-04, 5.9605e-08, 0.0000e+00, 6.5504e+04,        inf,       -inf],\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.HalfTensor([0.0001, 3e-8, 1e-8, 65519, 65520, -65520])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda [torch.float32, torch.float16]\n"
     ]
    }
   ],
   "source": [
    "# Sample random scalar data at fp32 and fp16\n",
    "nsamples = 20000\n",
    "nfeat = 1000\n",
    "device = 'cuda'\n",
    "\n",
    "cx = torch.randn([nsamples, nfeat], device=device)\n",
    "cy = torch.randn([nsamples, nfeat], device=device)\n",
    "\n",
    "hx = cx.half()\n",
    "hy = cy.half()\n",
    "\n",
    "print(device, [cx.dtype, hx.dtype])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(input, target):\n",
    "    '''\n",
    "    measures the mean squared error (squared L2 norm) between\n",
    "    each element in the input `x` and target :`y`\n",
    "    '''\n",
    "    diff = (input - target)\n",
    "    return (diff**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x2aaab7edd160>\n",
      "mse(x, y) # @fp32\n",
      "setup: from __main__ import mse\n",
      "  889.86 us\n",
      "  1 measurement, 100 runs , 1 thread\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x2aabf518d2e0>\n",
      "mse(x, y) # @fp16\n",
      "setup: from __main__ import mse\n",
      "  462.66 us\n",
      "  1 measurement, 100 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "t0 = benchmark.Timer(\n",
    "    stmt='mse(x, y) # @fp32',\n",
    "    setup='from __main__ import mse',\n",
    "    globals={'x': cx, 'y': cy})\n",
    "\n",
    "t1 = benchmark.Timer(\n",
    "    stmt='mse(x, y) # @fp16',\n",
    "    setup='from __main__ import mse',\n",
    "    globals={'x': hx, 'y': hy})\n",
    "\n",
    "print(t0.timeit(100))\n",
    "print(t1.timeit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>cpu_time_total</th>\n",
       "      <th>cuda_time_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cudaLaunchKernel</th>\n",
       "      <td>3</td>\n",
       "      <td>1.58095e+06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudaDeviceSynchronize</th>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudaMemsetAsync</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>void at::native::vectorized_elementwise_kernel&lt;4, at::native::AddFunctor&lt;c10::Half&gt;, at::detail::Array&lt;char*, 3&gt; &gt;(int, at::native::AddFunctor&lt;c10::Half&gt;, at::detail::Array&lt;char*, 3&gt;)</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>void at::native::vectorized_elementwise_kernel&lt;4, at::native::(anonymous namespace)::pow_tensor_scalar_kernel_impl&lt;c10::Half, c10::Half&gt;(at::TensorIteratorBase&amp;, c10::Half)::{lambda(c10::Half)#1}, at::detail::Array&lt;char*, 2&gt; &gt;(int, at::native::(anonymous namespace)::pow_tensor_scalar_kernel_impl&lt;c10::Half, c10::Half&gt;(at::TensorIteratorBase&amp;, c10::Half)::{lambda(c10::Half)#1}, at::detail::Array&lt;char*, 2&gt;)</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>void at::native::reduce_kernel&lt;512, 1, at::native::ReduceOp&lt;c10::Half, at::native::MeanOps&lt;float, float&gt;, unsigned int, c10::Half, 4&gt; &gt;(at::native::ReduceOp&lt;c10::Half, at::native::MeanOps&lt;float, float&gt;, unsigned int, c10::Half, 4&gt;)</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Memset (Device)</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   count cpu_time_total  \\\n",
       "cudaLaunchKernel                                       3    1.58095e+06   \n",
       "cudaDeviceSynchronize                                  1            140   \n",
       "cudaMemsetAsync                                        1             12   \n",
       "void at::native::vectorized_elementwise_kernel<...     1              0   \n",
       "void at::native::vectorized_elementwise_kernel<...     1              0   \n",
       "void at::native::reduce_kernel<512, 1, at::nati...     1              0   \n",
       "Memset (Device)                                        1              0   \n",
       "\n",
       "                                                   cuda_time_total  \n",
       "cudaLaunchKernel                                                 0  \n",
       "cudaDeviceSynchronize                                            0  \n",
       "cudaMemsetAsync                                                  0  \n",
       "void at::native::vectorized_elementwise_kernel<...             218  \n",
       "void at::native::vectorized_elementwise_kernel<...             149  \n",
       "void at::native::reduce_kernel<512, 1, at::nati...              85  \n",
       "Memset (Device)                                                  2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with profile(activities=[ProfilerActivity.CUDA]) as prof:\n",
    "    with record_function(\"mse\"):\n",
    "        mse(hx, hy)\n",
    "\n",
    "df = pd.DataFrame({e.key:e.__dict__ for e in prof.key_averages()}).T\n",
    "df[['count', 'cpu_time_total', 'cuda_time_total']].sort_values(['cpu_time_total', 'cuda_time_total'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.841255187988281e-05"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mse(cx, cy) - mse(hx, hy)).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mem_usage():\n",
    "    \"\"\" Context Manager to measure and print GPU memory usage within its bounds\n",
    "        Example::\n",
    "            >>> with mem_usage():\n",
    "            >>>     cuda_operation()\n",
    "            GPU-RAM Usage: 100.000MB\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.clear()\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.clear()\n",
    "        self.mem = torch.cuda.max_memory_allocated()\n",
    "        return self\n",
    "\n",
    "    def clear(self):\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        self.mem = 0\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, exc_traceback):\n",
    "        mem = torch.cuda.max_memory_allocated() - self.mem\n",
    "        print(f'GPU-RAM Usage: {mem / 1024**2:.3f}MB')\n",
    "        self.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU-RAM Usage: 381.471MB\n"
     ]
    }
   ],
   "source": [
    "var = cx.clone()\n",
    "var.requires_grad = True\n",
    "\n",
    "with mem_usage():\n",
    "    mse(var, cy).backward()\n",
    "    var.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU-RAM Usage: 190.736MB\n"
     ]
    }
   ],
   "source": [
    "var = hx.clone()\n",
    "var.requires_grad = True\n",
    "\n",
    "with mem_usage():\n",
    "    mse(var, hy).backward()\n",
    "    var.grad = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Mixed Precision on a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "def get_model_and_data(batch_size=64):\n",
    "    n_classes = 1000\n",
    "    imgs = torch.randn(batch_size, 3, 224, 224, device=device)\n",
    "    targets = torch.randint(n_classes, (batch_size, ), device=device)\n",
    "\n",
    "    model = models.densenet201(pretrained=False).to(device)\n",
    "    return model, imgs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FP32 Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/daint/UES/jenkins/7.0.UP02-20.11/gpu/easybuild/software/PyTorch/1.9.0-CrayGNU-20.11/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "516 ms ± 10.6 ms per loop (mean ± std. dev. of 10 runs, 1 loop each)\n",
      "GPU-RAM Usage: 12583.652MB\n"
     ]
    }
   ],
   "source": [
    "with mem_usage():\n",
    "    model, imgs, targets = get_model_and_data(batch_size=64)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "    def fp32_step():\n",
    "        # Runs the forward pass\n",
    "        out = model(imgs).log_softmax(dim=1)\n",
    "        loss = F.nll_loss(out, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "    %timeit -r10 -n1 fp32_step()\n",
    "\n",
    "    # clean up\n",
    "    del model; del optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Autocast and GradScaler\n",
    "\n",
    "- [amp.autocast](https://pytorch.org/docs/stable/amp.html#autocasting) allows regions of your script to run in mixed precision.\n",
    "- [Gradient scaling](https://pytorch.org/docs/stable/amp.html#gradient-scaling) helps prevent gradients with small magnitudes from flushing to zero (“underflowing”) when training with mixed precision.\n",
    "\n",
    "**Ops that can autocast to float16**\n",
    "- addbmm, addmm, addmv, addr, baddbmm, bmm, chain_matmul, multi_dot, conv1d, conv2d, conv3d, conv_transpose1d, conv_transpose2d, conv_transpose3d, GRUCell, linear, LSTMCell, matmul, mm, mv, prelu, RNNCell\n",
    "\n",
    "**Ops that can autocast to float32**\n",
    "- acos, asin, binary_cross_entropy_with_logits, cosh, cosine_embedding_loss, cdist, cosine_similarity, cross_entropy, cumprod, cumsum, dist, erfinv, exp, expm1, group_norm, hinge_embedding_loss, kl_div, l1_loss, layer_norm, log, log_softmax, log10, log1p, log2, margin_ranking_loss, mse_loss, multilabel_margin_loss, multi_margin_loss, nll_loss, norm, normalize, pdist, poisson_nll_loss, pow, prod, reciprocal, rsqrt, sinh, smooth_l1_loss, soft_margin_loss, softmax, softmin, softplus, sum, renorm, tan, triplet_margin_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944 ms ± 741 µs per loop (mean ± std. dev. of 10 runs, 1 loop each)\n",
      "GPU-RAM Usage: 12748.771MB\n"
     ]
    }
   ],
   "source": [
    "with mem_usage():\n",
    "    model, imgs, targets = get_model_and_data(batch_size=128)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def amp_step():\n",
    "        # Runs the forward pass under autocast.\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out = model(imgs).log_softmax(dim=1)\n",
    "            loss = F.nll_loss(out, targets)\n",
    "\n",
    "        # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
    "        # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n",
    "        # otherwise, optimizer.step() is skipped.\n",
    "        scaler.step(optimizer)\n",
    "\n",
    "        # Updates the scale for next iteration.\n",
    "        scaler.update()\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "    %timeit -r10 -n1 amp_step()\n",
    "\n",
    "    # clean up\n",
    "    del model; del optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NVIDIA Apex AMP\n",
    "\n",
    "Check https://nvidia.github.io/apex/amp.html?highlight=initialize#apex.amp.initialize for additional options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "945 ms ± 1.2 ms per loop (mean ± std. dev. of 10 runs, 1 loop each)\n",
      "GPU-RAM Usage: 12709.962MB\n"
     ]
    }
   ],
   "source": [
    "with mem_usage():\n",
    "    model, imgs, targets = get_model_and_data(batch_size=128)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # Allow Amp to perform casts as required by the opt_level\n",
    "    model, optimizer = apex.amp.initialize(model, optimizer, opt_level=\"O2\")\n",
    "\n",
    "    def amp_step():\n",
    "        out = model(imgs).log_softmax(dim=1)\n",
    "        loss = F.nll_loss(out, targets)\n",
    "\n",
    "        # loss.backward() becomes:\n",
    "        with apex.amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    %timeit -r10 -n1 amp_step()\n",
    "\n",
    "    # clean up\n",
    "    del model; del optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<mark>Exercise</mark>:\n",
    "\n",
    "1. Check the max batch size you can use densenet201 on a P100 with without AMP\n",
    "1. Go back to your distributed ImageNet training and add PyTorch's native AMP support to the training loop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepspeed",
   "language": "python",
   "name": "deepspeed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
