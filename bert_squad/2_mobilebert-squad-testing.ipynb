{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9abf6f5f-3396-4285-89ec-4b1709f6553c",
   "metadata": {},
   "source": [
    "# Fine-tuning a MobileBERT model for Q&A with the SQuAD dataset\n",
    "\n",
    "We are going to fine-tune [MobileBERT implemented by HuggingFace](https://huggingface.co/docs/transformers/model_doc/mobilebert) for the text-extraction task on the [The Stanford Question Answering Dataset (SQuAD)](https://rajpurkar.github.io/SQuAD-explorer/).\n",
    "\n",
    "The data is composed by a set of questions and paragraphs that contain the answers.\n",
    "The model will be trained to locate the answer in the context by giving the positions where the answer starts and ends.\n",
    "\n",
    "In this notebook we are going to evaluate the model from a checkpoint we already obtained.\n",
    "\n",
    "More info:\n",
    "- [Glossary - HuggingFace docs](https://huggingface.co/transformers/glossary.html#model-inputs)\n",
    "- [BERT NLP â€” How To Build a Question Answering Bot](https://towardsdatascience.com/bert-nlp-how-to-build-a-question-answering-bot-98b1d1594d7b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a9fef8-4780-4779-89de-662eb014d8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, MobileBertForQuestionAnswering\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08765c80-6338-4dfa-97ce-cdd5adbc26af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.utils import disable_progress_bar\n",
    "from datasets import disable_caching\n",
    "\n",
    "\n",
    "disable_progress_bar()\n",
    "disable_caching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f44b4b-935a-4f86-b4fc-87d2036fd215",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_checkpoint = 'google/mobilebert-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728089ed-5d62-491a-b623-d6928df88823",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_checkpoint)\n",
    "model = MobileBertForQuestionAnswering.from_pretrained(hf_model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0711c779-f6d0-4c6b-b127-286d8a4225fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = load_dataset('squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ebb191-90f2-48e9-bf6c-80982682a893",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 300\n",
    "\n",
    "def tokenize_dataset(squad_example, tokenizer=tokenizer):\n",
    "    \"\"\"Tokenize the text in the dataset and convert\n",
    "    the start and ending positions of the answers\n",
    "    from text to tokens\"\"\"\n",
    "    max_len = MAX_SEQ_LEN\n",
    "    context = squad_example['context']\n",
    "    answer_start = squad_example['answers']['answer_start'][0]\n",
    "    answer = squad_example['answers']['text'][0]\n",
    "    squad_example_tokenized = tokenizer(\n",
    "        context, squad_example['question'],\n",
    "        padding='max_length',\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "    )\n",
    "    token_start = len(tokenizer.tokenize(context[:answer_start + 1]))\n",
    "    token_end = len(tokenizer.tokenize(answer)) + token_start\n",
    "\n",
    "    squad_example_tokenized['start_token_idx'] = token_start\n",
    "    squad_example_tokenized['end_token_idx'] = token_end\n",
    "\n",
    "    return squad_example_tokenized\n",
    "\n",
    "\n",
    "def filter_samples_by_max_seq_len(squad_example):\n",
    "    \"\"\"Fliter out the samples where the answers are\n",
    "    not within the first `MAX_SEQ_LEN` tokens\"\"\"\n",
    "    max_len = MAX_SEQ_LEN\n",
    "    answer_start = squad_example['answers']['answer_start'][0]\n",
    "    answer = squad_example['answers']['text'][0]\n",
    "    token_start = len(tokenizer.tokenize(squad_example['context'][:answer_start]))\n",
    "    token_end = len(tokenizer.tokenize(answer)) + token_start\n",
    "    if token_end < max_len:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd08918-e54c-42c1-9c91-188cba6fd416",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered = hf_dataset.filter(\n",
    "    filter_samples_by_max_seq_len,\n",
    "    num_proc=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce11e8a3-eef0-4fed-8ce7-8c7756138821",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tok = dataset_filtered.map(\n",
    "    tokenize_dataset,\n",
    "    remove_columns=hf_dataset['train'].column_names,\n",
    "    num_proc=12,\n",
    ")\n",
    "dataset_tok.set_format('pt')\n",
    "dataset_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c871cd-568c-4746-b26f-4fb8fecfb475",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataloader = DataLoader(\n",
    "    dataset_tok['validation'],\n",
    "    shuffle=True,   # shuffle to print different predictions\n",
    "    batch_size=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219328a3-44ca-4f8b-926a-071f930ae845",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7a3135-cd96-4c5c-88cb-447f4254bb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(\n",
    "    torch.load('mobilebertforaq_trained_thu',\n",
    "               map_location=torch.device('cpu'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5e61e3-df88-4316-8bbf-c1f3ab619a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(eval_dataloader):\n",
    "    # evaluate the model\n",
    "    outputs = model(\n",
    "        input_ids=batch['input_ids'],\n",
    "        token_type_ids=batch['token_type_ids'],\n",
    "        attention_mask=batch['attention_mask']\n",
    "    )\n",
    "    \n",
    "    # obtain the predicted start and end possitions and\n",
    "    # apply a softmax to it\n",
    "    pred_start = F.softmax(outputs.start_logits, dim=-1)\n",
    "    pred_end   = F.softmax(outputs.end_logits,   dim=-1)\n",
    "    \n",
    "    # loop over the batch\n",
    "    for context_tokens, start_ref, end_ref, start, end, in zip(batch['input_ids'],\n",
    "                                                               batch['start_token_idx'], batch['end_token_idx'],\n",
    "                                                               pred_start, pred_end):\n",
    "        context_text = tokenizer.decode(context_tokens, skip_special_tokens=True,\n",
    "                                        clean_up_tokenization_spaces=True)\n",
    "        # find the max class that the softmax gives\n",
    "        start = torch.argmax(start)\n",
    "        end = torch.argmax(end)\n",
    "        \n",
    "        # predicted answer\n",
    "        answer_tokens = context_tokens[start:end]\n",
    "        answer_text = tokenizer.decode(answer_tokens, skip_special_tokens=True,\n",
    "                                       clean_up_tokenization_spaces=True)\n",
    "        \n",
    "        # reference answers\n",
    "        answer_tokens_ref = context_tokens[start_ref:end_ref]\n",
    "        answer_text_ref = tokenizer.decode(answer_tokens_ref, skip_special_tokens=True,\n",
    "                                           clean_up_tokenization_spaces=True)\n",
    "                \n",
    "        print(f'* {context_text}\\n')\n",
    "        print(f'[robot] {answer_text}')\n",
    "        print(f'[ ref ] {answer_text_ref}\\n')\n",
    "        print('---')\n",
    "        \n",
    "    if i > 10:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2024",
   "language": "python",
   "name": "ml2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
