{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d90abbe-bcf5-482c-9a6a-2b8f074e3753",
   "metadata": {},
   "source": [
    "# Crossentropy\n",
    "\n",
    "## Objective:\n",
    "In this notebook, we explore the relationship between softmax probabilities and crossentropy loss in the context of a classification task. We aim to visually analyze how well our model is predicting class probabilities and how the crossentropy loss reflects the alignment between predicted and true distributions.\n",
    "\n",
    "## Background:\n",
    "In classification problems, the crossentropy loss is a common choice for measuring the dissimilarity between predicted and true probability distributions. For a binary classification scenario, the crossentropy loss (log loss) is calculated as follows:\n",
    "\n",
    "$$\n",
    "\\text{Crossentropy Loss} = - \\sum_{i}^{N} \\left( y_i \\cdot \\log(\\hat{y}_i) + (1 - y_i) \\cdot \\log(1 - \\hat{y}_i) \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $N$ is the number of classes (2 in binary classification).\n",
    "- $y_i$ is the true probability distribution (1 for the true class, 0 for others).\n",
    "- $\\hat{y}_i$ is the predicted probability for class $i$.\n",
    "\n",
    "We will visualize the softmax probabilities and the corresponding crossentropy loss for each reference class. This allows us to gain insights into how our model is making predictions and which classes contribute the most to the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5859433-2a19-4868-a618-f63e5bc42d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1642c6-c55e-4a11-82a5-f51ec1191534",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f498c985-ca1d-4e55-8f6b-a19288e8d1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random logits\n",
    "logits = torch.randn((1, num_classes))\n",
    "\n",
    "# Apply softmax\n",
    "probabilities = F.softmax(logits, dim=1)\n",
    "\n",
    "crossentropy = torch.nn.CrossEntropyLoss()\n",
    "losses = torch.tensor([crossentropy(logits, torch.tensor((i,))) for i in range(num_classes)])\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 4)\n",
    "\n",
    "# Plot the original logits and probabilities\n",
    "fig, ax1 = plt.subplots()  # 1, 3, figsize=(10, 4))\n",
    "\n",
    "# Plot original logits\n",
    "ax1.bar( range(num_classes), probabilities.squeeze(), color='deepskyblue', label='probabilities')\n",
    "ax1.bar( range(num_classes), losses / (-num_classes * losses.max()), color='tomato', label='crossentropy (scaled)')\n",
    "ax1.plot(range(num_classes), logits.squeeze()  / (num_classes * torch.abs(logits).max()), '.-', color='k', label='logits (scaled)')\n",
    "ax1.set_xlabel('Class')\n",
    "ax1.set_ylabel('Probabilities')\n",
    "ax1.set_ylim([-0.12 / (0.1 * num_classes), probabilities.max() + (0.01 / (0.1 * num_classes))])\n",
    "ax1.grid(ls=':', alpha=0.5)\n",
    "ax1.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-2.3.0",
   "language": "python",
   "name": "pytorch-2.3.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
