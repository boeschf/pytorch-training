{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9abf6f5f-3396-4285-89ec-4b1709f6553c",
   "metadata": {},
   "source": [
    "# MobileBERT for Question Answering on the SQuAD dataset\n",
    "\n",
    "### 2. Fine-tuning the model\n",
    "\n",
    "In these notebooks we are going use [MobileBERT implemented by HuggingFace](https://huggingface.co/docs/transformers/model_doc/mobilebert) on the question answering task by text-extraction on the [The Stanford Question Answering Dataset (SQuAD)](https://rajpurkar.github.io/SQuAD-explorer/). The data is composed by a set of questions and paragraphs that contain the answers. The model will be trained to locate the answer in the context by giving the positions where the answer starts and ends.\n",
    "\n",
    "In this notebook we are going to Fine-tuning the model.\n",
    "\n",
    "More info from HuggingFace docs:\n",
    "- [Question Answering](https://huggingface.co/tasks/question-answering)\n",
    "- [Glossary](https://huggingface.co/transformers/glossary.html#model-inputs)\n",
    "- [Question Answering chapter of NLP course](https://huggingface.co/learn/nlp-course/chapter7/7?fw=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a9fef8-4780-4779-89de-662eb014d8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, MobileBertForQuestionAnswering\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08765c80-6338-4dfa-97ce-cdd5adbc26af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.utils import disable_progress_bar\n",
    "from datasets import disable_caching\n",
    "\n",
    "\n",
    "disable_progress_bar()\n",
    "disable_caching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f44b4b-935a-4f86-b4fc-87d2036fd215",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_checkpoint = 'google/mobilebert-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728089ed-5d62-491a-b623-d6928df88823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the tokenizer that was used for pretraining that model\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26b500b-f80d-4e4e-940b-0076db7c5056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model\n",
    "model = MobileBertForQuestionAnswering.from_pretrained(hf_model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de5343b-999d-49ed-8565-e23aba38cd41",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "When instantiating model, there's a red message coming up. What does it mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0711c779-f6d0-4c6b-b127-286d8a4225fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = load_dataset('squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ebb191-90f2-48e9-bf6c-80982682a893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data\n",
    "# Find more info about this in the notebook about exploring the dataset\n",
    "\n",
    "MAX_SEQ_LEN = 300\n",
    "\n",
    "def tokenize_dataset(squad_example, tokenizer=tokenizer):\n",
    "    \"\"\"Tokenize the text in the dataset and convert\n",
    "    the start and ending positions of the answers\n",
    "    from text to tokens\"\"\"\n",
    "    max_len = MAX_SEQ_LEN\n",
    "    context = squad_example['context']\n",
    "    answer_start = squad_example['answers']['answer_start'][0]\n",
    "    answer = squad_example['answers']['text'][0]\n",
    "    squad_example_tokenized = tokenizer(\n",
    "        context, squad_example['question'],\n",
    "        padding='max_length',\n",
    "        max_length=max_len,\n",
    "        truncation='only_first',\n",
    "    )\n",
    "    token_start = len(tokenizer.tokenize(context[:answer_start + 1]))\n",
    "    token_end = len(tokenizer.tokenize(answer)) + token_start\n",
    "\n",
    "    squad_example_tokenized['start_token_idx'] = token_start\n",
    "    squad_example_tokenized['end_token_idx'] = token_end\n",
    "\n",
    "    return squad_example_tokenized\n",
    "\n",
    "\n",
    "def filter_samples_by_max_seq_len(squad_example):\n",
    "    \"\"\"Fliter out the samples where the answers are\n",
    "    not within the first `MAX_SEQ_LEN` tokens\"\"\"\n",
    "    max_len = MAX_SEQ_LEN\n",
    "    answer_start = squad_example['answers']['answer_start'][0]\n",
    "    answer = squad_example['answers']['text'][0]\n",
    "    token_start = len(tokenizer.tokenize(squad_example['context'][:answer_start]))\n",
    "    token_end = len(tokenizer.tokenize(answer)) + token_start\n",
    "    return token_end < max_len\n",
    "\n",
    "dataset_filtered = hf_dataset.filter(\n",
    "    filter_samples_by_max_seq_len,\n",
    "    num_proc=24,\n",
    ")\n",
    "\n",
    "dataset_tok = dataset_filtered.map(\n",
    "    tokenize_dataset,\n",
    "    remove_columns=hf_dataset['train'].column_names,\n",
    "    num_proc=24,\n",
    ")\n",
    "dataset_tok.set_format('pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c871cd-568c-4746-b26f-4fb8fecfb475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use batch size 256 for a fast training\n",
    "batch_size = 256\n",
    "\n",
    "# Define a PyTorch Dataloader for the train set\n",
    "train_dataloader = DataLoader(\n",
    "    dataset_tok['train'],\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5e7095-724e-4653-88bc-74f283776de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to the GPU 0\n",
    "device = 0\n",
    "model.to(device)\n",
    "\n",
    "# Set the model for training\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e375d2f-d159-489f-b036-62332a64d0a4",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We are going to train for two epocs. We will use a different learning rate values in each epoch:\n",
    " - epoch 1: `lr = 2e-4` (to move fast on the loss function over the parameter space)\n",
    " - epoch 2: `lr = 2e-5` (to avoid jumping around and start converging towards a minimum)\n",
    "\n",
    "We will do this manually:\n",
    " - Run epoch one\n",
    " - Redifine the optimizer with the new learning rate and run again the training\n",
    "\n",
    "We should aim to loss values around 0.6, which will ensure \"decent\" predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49addf3-f0f7-4262-913a-9710ead1d2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer using \"AdamW\" (Adam with decoupled weight decay)\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874ae3c3-437b-48a6-9121-5bee67b78f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(loss):\n",
    "    \"\"\"Utility function for plotting\"\"\"\n",
    "\n",
    "    return loss.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb04b5ab-2714-4a0f-aede-179b1b20275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "\n",
    "for epoch in range(1):\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        optim.zero_grad()  # reset automatic differentiation record\n",
    "        # evaluate the model and pass the output references (start_token_idx and end_token_idx)\n",
    "        outputs = model(input_ids=batch['input_ids'].to(device),\n",
    "                        token_type_ids=batch['token_type_ids'].to(device),\n",
    "                        attention_mask=batch['attention_mask'].to(device),\n",
    "                        start_positions=batch['start_token_idx'].to(device),\n",
    "                        end_positions=batch['end_token_idx'].to(device))        \n",
    "        loss = outputs[0]          # obtain the loss from the model output (specific of HugginFace's API)\n",
    "        history.append(log(loss))  # [not part of the traing] keep values for plotting later\n",
    "        loss.backward()    # backpropagation -> gradients\n",
    "        optim.step()       # update weights with the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6500b1-b2f0-4e2a-937e-2af917b59be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history, 'r-')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Steps')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40befe5-9635-42c5-91d9-3baeafe76c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the mode to disk\n",
    "torch.save(model.state_dict(), 'mobilebertqa_ft')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a67584-b42d-403a-bc82-cd490843fb20",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "\n",
    "When your model is trained, run the notebook `3_mobilebert-squad-testing.ipynb` to test it on the validation set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
