{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of a Simple Stochastic Gradient Descent\n",
    "\n",
    "Here we visualize the minimization of the loss with the SGD algorithm in its variants vanilla GD, batch SGD and minibatch SGD. For this we consider a linear model with only one weight and one bias (the slope and the offset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Let's create the dataset. We generate a random vector $x \\in [-0.5, 0.5]$ and evaluate it in a linear function $y = 2x$. We add some noise to $y$ and that give us $y \\in [-1.5, 1.5]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDataset(Dataset):\n",
    "    '''The training data is generated from the linear\n",
    "    function\n",
    "              y = m * x + n\n",
    "    where `m` is the slope and `n` is the offset.\n",
    "    Random noise in the range `[-0.5, 0.5]` is added\n",
    "    to the function value `y`.\n",
    "    '''\n",
    "    ref_slope = 2.0\n",
    "    ref_offset = 0.0\n",
    "    nsamples = 1024\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.FloatTensor(1,).uniform_(-0.5, 0.5)\n",
    "        y = self.ref_slope * x + self.ref_offset\n",
    "        noise = torch.FloatTensor(1,).uniform_(-0.5, 0.5)\n",
    "        return (x, y + noise)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = LinearDataset()\n",
    "batch_size_per_gpu = 128\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size_per_gpu,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = []\n",
    "y_plot = []\n",
    "for x, y in train_loader:\n",
    "    x_plot.append(x.numpy().flatten())\n",
    "    y_plot.append(y.numpy().flatten())\n",
    "    \n",
    "x_plot = np.array(x_plot).reshape(8 * 128)\n",
    "y_plot = np.array(y_plot).reshape(8 * 128)\n",
    "\n",
    "plt.plot(x_plot, y_plot, '.')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Defining the model and choosing an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Linear(1, 1, bias=True, device=device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)   #, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(x.to(device))\n",
    "        loss = F.mse_loss(y_hat, y.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        history.append([model.weight.item(), model.bias.item(), loss.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_hist = np.array(history)[:, 0]\n",
    "offset_hist = np.array(history)[:, 1]\n",
    "loss_hist = np.array(history)[:, 2]\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 4)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_hist[10:], 'r.-')\n",
    "plt.xlabel('Training steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x_plot, y_plot, '.', alpha=.3)\n",
    "plt.plot(x_plot, slope_hist[0]  * x_plot + offset_hist[0],  'r-', label='model (initial step)')\n",
    "plt.plot(x_plot, slope_hist[-1] * x_plot + offset_hist[-1], 'g-', label='model (trained)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (6, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_field(m, n, xref, yref):\n",
    "    '''Utility function for ploting the loss'''\n",
    "    return np.mean(np.square(yref - m * xref - n ))\n",
    "\n",
    "_m = np.arange(-0, 4.01, 0.1)\n",
    "_n = np.arange(-0.5, 0.51, 0.1)\n",
    "M, N = np.meshgrid(_m, _n)\n",
    "\n",
    "Z = np.zeros(M.shape)\n",
    "for i in range(M.shape[0]):\n",
    "    for j in range(M.shape[1]):\n",
    "        Z[i, j] = loss_function_field(M[i, j], N[i, j],\n",
    "                                      x_plot, y_plot)\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 7.0)\n",
    "\n",
    "cp = plt.contour(M, N, Z, 15, vmin=Z.min(), vmax=Z.max(), alpha=0.99, colors='k', linestyles='--')\n",
    "plt.contourf(M, N, Z, vmin=Z.min(), vmax=Z.max(), alpha=0.8, cmap=plt.cm.RdYlBu_r)\n",
    "plt.clabel(cp, cp.levels[:6])\n",
    "plt.colorbar()\n",
    "m = slope_hist[-1]\n",
    "n = offset_hist[-1]\n",
    "plt.plot(slope_hist, offset_hist, '.-', lw=2, c='k')\n",
    "plt.plot([train_set.ref_slope], [train_set.ref_offset], 'rx', ms=10)\n",
    "plt.xlim([_m.min(), _m.max()])\n",
    "plt.ylim([_n.min(), _n.max()])\n",
    "plt.xlabel('Slope')\n",
    "plt.ylabel('Offset')\n",
    "plt.show()\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (6, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>Exercise 1</mark>: Try different batch sizes:\n",
    " * `batch_size = 1000` (The whole dataset)\n",
    " * `batch_size = 100` (Something between 1 and the size of whole dataset.)\n",
    " * `batch_size = 1`\n",
    "\n",
    "1. Check the SGD trajectories. Why are they different?\n",
    "2. The case `batch_size = 1` is quite slow compared to `batch_size = 100` and `batch_size = 1000`. Why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpcpython2022",
   "language": "python",
   "name": "hpcpython2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
