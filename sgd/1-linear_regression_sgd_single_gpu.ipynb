{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of a Simple Stochastic Gradient Descent\n",
    "\n",
    "Here we visualize the minimization of the loss with the SGD algorithm in its variants vanilla GD, batch SGD and minibatch SGD. For this we consider a linear model with only one weight and one bias (the slope and the offset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Let's create the dataset. We generate a random vector $x \\in [-0.5, 0.5]$ and evaluate it in a linear function $y = 2x$. We add some noise to $y$ and that give us $y \\in [-1.5, 1.5]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDataset(Dataset):\n",
    "    '''The training data is generated from the linear\n",
    "    function\n",
    "              y = m * x + n\n",
    "    where `m` is the slope and `n` is the offset.\n",
    "    Random noise in the range `[-0.5, 0.5]` is added\n",
    "    to the function value `y`.\n",
    "    '''\n",
    "    ref_slope = 2.0\n",
    "    ref_offset = 0.0\n",
    "    nsamples = 1024\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.FloatTensor(1,).uniform_(-0.5, 0.5)\n",
    "        y = self.ref_slope * x + self.ref_offset\n",
    "        noise = torch.FloatTensor(1,).uniform_(-0.5, 0.5)\n",
    "        return (x, y + noise)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = LinearDataset()\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how the data looks like\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = []\n",
    "y_plot = []\n",
    "for x, y in train_loader:\n",
    "    x_plot.append(x.numpy())\n",
    "    y_plot.append(y.numpy())\n",
    "    \n",
    "x_plot = np.array(x_plot).flatten()\n",
    "y_plot = np.array(y_plot).flatten()\n",
    "\n",
    "plt.plot(x_plot, y_plot, '.')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Defining the model and choosing an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Linear(1, 1, bias=True, device=device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# num_epochs = 50\n",
    "# history = []\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     for x, y in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         y_hat = model(x.to(device))\n",
    "#         loss = F.mse_loss(y_hat, y.to(device))\n",
    "#         history.append(record_for_plotting(model, loss))\n",
    "#         loss.backward()\n",
    "#         optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(model, loss):\n",
    "    \"\"\"utility function for plotting\"\"\"\n",
    "    return(model.weight.item(),\n",
    "           model.bias.item(),\n",
    "           loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50                                   #\n",
    "history = []                                      #\n",
    "                                                  #\n",
    "for epoch in range(num_epochs):                   #   loop over epochs:\n",
    "    for x, y in train_loader:                     #       loop over batches:  -> (x, y)\n",
    "        optimizer.zero_grad()                     #            * reset automatic differentiation record\n",
    "        y_hat = model(x.to(device))               #            * evaluate the model in a batch -> y_hat\n",
    "        loss = F.mse_loss(y_hat, y.to(device))    #            * evaluate the loss function with the obtained y_hat and y\n",
    "        history.append(log(model, loss))          #            [not part of the traing] keep values for plotting later\n",
    "        loss.backward()                           #            * backpropagation -> gradients\n",
    "        optimizer.step()                          #            * update weights with the gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_hist = np.array(history)[:, 0]\n",
    "offset_hist = np.array(history)[:, 1]\n",
    "loss_hist = np.array(history)[:, 2]\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 4)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_hist[10:], 'r.-')\n",
    "plt.xlabel('Training steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x_plot, y_plot, '.', alpha=.3)\n",
    "plt.plot(x_plot, slope_hist[0]  * x_plot + offset_hist[0],  'r-', label='model (initial step)')\n",
    "plt.plot(x_plot, slope_hist[-1] * x_plot + offset_hist[-1], 'g-', label='model (trained)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (6, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_field(m, n, xref, yref):\n",
    "    '''Utility function for ploting the loss'''\n",
    "    return np.mean(np.square(yref - m * xref - n ))\n",
    "\n",
    "_m = np.arange( -0.5, 4.51, 0.1)\n",
    "_n = np.arange(-1., 1.01, 0.1)\n",
    "M, N = np.meshgrid(_m, _n)\n",
    "\n",
    "Z = np.zeros(M.shape)\n",
    "for i in range(M.shape[0]):\n",
    "    for j in range(M.shape[1]):\n",
    "        Z[i, j] = loss_function_field(M[i, j], N[i, j],\n",
    "                                      x_plot, y_plot)\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (16.0, 5.2)\n",
    "\n",
    "cp = plt.contour(M, N, Z, 12, vmin=Z.min(), vmax=Z.max(), alpha=0.99, colors='k', linestyles='--')\n",
    "plt.contourf(M, N, Z, 60, vmin=Z.min(), vmax=Z.max(), alpha=0.8, cmap=plt.cm.RdYlBu_r)\n",
    "plt.clabel(cp, cp.levels[:6])\n",
    "plt.colorbar()\n",
    "m = slope_hist[-1]\n",
    "n = offset_hist[-1]\n",
    "plt.plot(slope_hist, offset_hist, '.-', lw=2, c='k')\n",
    "plt.plot([train_set.ref_slope], [train_set.ref_offset], 'rx', ms=10)\n",
    "plt.xlim([_m.min(), _m.max()])\n",
    "plt.ylim([_n.min(), _n.max()])\n",
    "plt.xlabel('Slope')\n",
    "plt.ylabel('Offset')\n",
    "plt.show()\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (6, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Exercise]** Try different batch sizes:\n",
    " * `batch_size = 1024` - The whole dataset\n",
    " * `batch_size = 128`  - Something between 1 and the size of the whole dataset\n",
    " * `batch_size = 1`\n",
    "\n",
    "1. Check the SGD trajectories. Why are they different?\n",
    "2. The case `batch_size = 1` is quite slow compared to `batch_size = 100` and `batch_size = 1024`. Why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2024",
   "language": "python",
   "name": "ml2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
