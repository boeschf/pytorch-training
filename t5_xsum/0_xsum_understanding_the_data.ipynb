{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text summarization with T5 on XSum\n",
    "\n",
    "We are going to fine-tune the [T5 model, implemented by HuggingFace](https://huggingface.co/t5-small), for text summarization on the [Extreme Summarization (XSum)](https://huggingface.co/datasets/xsum) dataset.\n",
    "The data is composed by news articles and the corresponding summaries.\n",
    "\n",
    "We will be using the following model sizes available from HuggingFace\n",
    "\n",
    "| Variant                                     |   Parameters    |\n",
    "|:-------------------------------------------:|----------------:|\n",
    "| [T5-small](https://huggingface.co/t5-small) |    60,506,624   | \n",
    "| [T5-large](https://huggingface.co/t5-large) |   737,668,096   | \n",
    "| [T5-3b](https://huggingface.co/t5-3b)       | 2,851,598,336   | \n",
    "\n",
    "\n",
    "More info:\n",
    "* This notebooks is based on the script [run_summarization_no_trainer.py](https://github.com/huggingface/transformers/blob/v4.12.5/examples/pytorch/summarization/run_summarization_no_trainer.py) from HuggingFace\n",
    "* [T5 on HuggingFace docs](https://huggingface.co/transformers/model_doc/t5.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datasets\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, DataCollatorForSeq2Seq\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.utils import disable_progress_bar\n",
    "from datasets import disable_caching\n",
    "\n",
    "\n",
    "disable_progress_bar()\n",
    "disable_caching()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset xsum (/users/sarafael/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934)\n"
     ]
    }
   ],
   "source": [
    "hf_dataset = load_dataset('xsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 204045\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 11332\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 11334\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = 33609  # twin peaks\n",
    "# sample = 192550 # twin peaks\n",
    "sample = 188948   # blues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'15575668'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset['train']['id'][sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BB King was hailed as one of the greatest blues musicians of all time.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset['train']['summary'][sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'His vibrato style of playing influenced a generation of rock and blues guitarists, including Eric Clapton, Mike Bloomfield and Stevie Ray Vaughan.\\nRolling Stone magazine once ranked BB King in third place in its list of the 100 greatest guitarists of all time, just below Jimi Hendrix and Duane Allman.\\nHis output crossed musical barriers, from jazz and blues to mainstream pop.\\nHe was born Riley B King in Indianola, Mississippi, on 16 September 1925. His parents were sharecroppers and, as a young boy, he helped them work in the fields.\\nThe family struggled. \"When you live in a house that you can always peek out of and see what kind of day it is,\" King later said, \"you\\'re not doing so well.\"\\nThe sound of his co-workers hollering the blues was his first introduction to the style of music that he was to help take from a purely black American audience into the mainstream.\\nHe bought his first guitar when he was barely a teenager so he could play at church services. In 1947 he moved to Memphis where he busked on the streets until he found work as a radio disc jockey at station WDIA.\\nHe was introduced as \"The Beale Street Blues Boy\", later shortened to BB. He also built a reputation as a guitarist in the Beale Street blues clubs.\\nHe later said: \"I\\'ve said that playing the blues is like having to be black twice.\"\\nIt was while playing in one of the clubs that a fight broke out over a woman, causing a fire. After rushing out of the wooden building, he realised that he had left his guitar behind.\\nHe risked his life by going back in to rescue his instrument. He named it after the woman whose charms had been behind the trouble: Lucille.\\nAfter making his first record in 1949, he went on to top the rhythm and blues charts two years later with Three O\\'Clock Blues. The song remained at number one for 17 weeks.\\nMany of his early recordings were produced by the legendary Sam Phillips who went on to found Sun Records.\\nOn the strength of this success, he was able to work across the US and he performed at such venues as the Apollo Theatre in Harlem, New York. Further hits included Sweet Black Angel, Rock Me Baby and Every Day I Have the Blues.\\nHe played more than 300 gigs on the so-called Chitlin\\' Circuit, the collection of performance venues in what were then racially segregated southern states where it was safe for black musicians to perform.\\nKing said: \"I have put up with more humiliation than I care to remember.\\n\"Touring a segregated America, forever being stopped and harassed by white cops hurt you most \\'cos you didn\\'t realise the damage. You hold it in.\"\\nIt was thanks to the influence of British bands such as the Yardbirds, the Animals and the Rolling Stones that white audiences, first in the UK and later in America, began to embrace the blues.\\nBB King began to be accepted in venues that had long been closed to black musicians. One of his more moving moments was when he was given a standing ovation by a mainly white audience at the Fillmore West theatre in San Francisco in 1968.\\nHe later recalled that he had berated his bus driver for bringing him to the wrong venue after seeing the overwhelmingly white faces in the queues of people waiting to get in.\\nThe same year, he made his first tour of Europe. He returned many times, becoming as popular there as at home.\\nHe had a UK top 20 hit with The Thrill is Gone in 1969, but his most successful single came with the band U2 in 1989 with When Love Comes To Town.\\nIn 2000 he collaborated with long-time fan, and blues purist, Eric Clapton on the album Riding with the King.\\nKing returned to Mississippi each year to visit his numerous children from a number of relationships.\\nHe once said: \"Ladies, friends and music - without those three, I wouldn\\'t wanna be here.\"\\nKing was still touring in his 80s, having played more than 15,000 live gigs during his career.\\nHe also made a point of playing regular concerts in prisons across the US.\\nKing was once asked what motivated him to keep up his punishing schedule of live performances.\\n\"I would like very much to make them happy,\" he replied. \"I want them, when they leave the venue, to say \\'I enjoyed myself\\'.\"'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset['train']['document'][sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model = 't5-small'\n",
    "t5_cache = os.path.join(os.getcwd(), 'cache')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    hf_model,\n",
    "    use_fast=True,\n",
    "    cache_dir=os.path.join(t5_cache, f'{hf_model}_tokenizer')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = tokenizer(\"What's up tokenizer!\", max_length=1024,\n",
    "                         padding=False, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [363, 31, 7, 95, 14145, 8585, 55, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What', \"'\", 's', 'up', 'token', 'izer', '!', '</s>']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(encoded_text['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "    encoded_text = tokenizer(\"What's up tokenizer!\", max_length=1024,\n",
    "                             padding=False, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [363, 31, 7, 95, 14145, 8585, 55, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):    \n",
    "    inputs = examples['document']\n",
    "    targets = examples['summary']\n",
    "    inputs = [f'summarize: {inp}' for inp in inputs]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024,\n",
    "                             padding=False, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=128,\n",
    "                           padding=False, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.03 s, sys: 394 ms, total: 1.42 s\n",
      "Wall time: 35.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "processed_datasets = hf_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=hf_dataset[\"train\"].column_names,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    "    num_proc=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 204045\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 11332\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 11334\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training Sequence to Sequence models, we need a special kind of data collator,\n",
    "# which will not only pad the inputs to the maximum length in the batch,\n",
    "# but also the labels.\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    label_pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "train_dataset = processed_datasets[\"train\"]\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=per_device_train_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "    if step > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1024])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([21603,    10,  1363,  ...,     0,     0,     0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1,  ..., 0, 0, 0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['attention_mask'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'summarize: Mr Fox, 54, from London, denies eight counts of indecent assault and two counts of sexual assault between 1988 and 2014. He said there was often \"horseplay\" with colleagues, involving \"piggybacks, tickling and squeezing\". But he told Westminster Magistrates\\' Court such behaviour was consensual. Mr Fox, who uses the nicknames Dr Fox and Foxy, became well known for presenting the chart show on Capital Radio, and was a judge on the ITV show Pop Idol between 2001 and 2003 alongside Simon Cowell. He joined Magic 105.4 in 2005, where he presents the breakfast show, Foxy in the Morning. He is currently not hosting the show. Giving evidence on Wednesday, Mr Fox said he had worked with \"hundreds\" of female colleagues during his career, but had never been accused of sexually inappropriate behaviour until last year. Under questioning from his defence counsel, Jonathan Caplan QC, he told the court his teams had kept their energy up during live broadcasts by playing loud music, dancing and creating makeshift obstacle courses. \"It sounds daft, but it was the way we, I, got myself going, and the team would join in very much as well. They would be part of that.\" He said his entire shifts, not just the on-air periods, constituted a \"full performance\" and colleagues would often make playful comments to one another. Mr Caplan asked: \"Could those comments become edgy or sexual?\" Mr Fox replied: \"Of course they could. Like any office. Some of it could be saucy, cheeky, over-the-top.\" Referring to the \"horseplay\" in the office, Mr Fox insisted he would never have engaged in such conduct if it was not consensual, \"otherwise it would create a bad atmosphere in the studio\". Asked whether the culture of radio had changed during the past 25 years, he said: \"I think the workplace in general, it\\'s changed, hasn\\'t it? \"Laws have changed. HR has changed. Political correctness has changed.\" He added: \"But I don\\'t think my behaviour has changed. I\\'m the same guy now, with the same morals, as I was then.\" One former colleague has accused the DJ of squeezing her breasts from behind. Asked about that, he said: \"That would be wholly wrong, totally disrespectful, and it\\'s not something I would do.\" The complainant, who cannot be named for legal reasons, has also said Mr Fox once pushed her over and simulated having sex with her. Addressing that allegation, he said people in his then-workplace often pretended to have been caught in compromising positions - it was a \"play-acting high jinks thing\", he said. Mr Fox told the court he had seen it \"many times\", but agreed that out of context, the \"Benny Hill-style\" comedy could have seemed inappropriate. Prosecutor John Price QC said: \"She says that when you did that to her, she did not consent to it. Are you in a position to dispute that?\" Mr Fox replied: \"I can\\'t get in (her) head.\" Mr Price continued: \"Did you ask her before you did it if it was okay?\" to which the defendant replied: \"No.\" Mr Fox also denied having sexual contact with a 15-year-old fan, including during a private tour of Capital Radio\\'s record library. \"It never happened,\" he said, although he did acknowledge he \"could have easily taken her for a tour\". Mr Fox agreed it would be \"quite wrong\" for an adult man to \"stick his tongue\" into the mouth of a teenage girl, as the woman alleged he had done in the station\\'s car park on another occasion. If a young fan asked him for a kiss, he would respond with \"a social kiss\" on the cheek, he said. The girl in question had, he added, \"become a little bit obsessed with me\". Asked why three unrelated individuals were accusing him of similar inappropriate conduct, Mr Fox replied: \"I don\\'t have an explanation at all.\" Mr Price said: \"Coincidence is ridiculous, isn\\'t it, as an explanation for these things?\" The DJ replied: \"I can\\'t explain those coincidences.\" The trial continues.</s>'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(batch['input_ids'][0][batch['attention_mask'][0]==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6920, 17906,  7547,    65,  1219,     3,     9,  1614,     3,    88,\n",
       "         5908,    16,    96,  7348,    75,    63,   121,  7916,    44,   161,\n",
       "            6,    68, 11244,   263,   581,   376,    16,  1412,   130,     8,\n",
       "          166,    16,   112,  2838,    18,  1201,  1415,     5,     1,     0,\n",
       "            0,     0,     0,     0])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DJ Neil Fox has told a court he engaged in \"saucy\" behaviour at work, but complaints made against him in 2014 were the first in his 29-year career.</s><pad><pad><pad><pad><pad>',\n",
       " \"Tottenham midfielder Dele Alli has been ruled out of England's World Cup qualifier with Scotland and friendly against Spain after suffering a knee injury in training.</s><pad><pad><pad><pad><pad><pad><pad>\",\n",
       " \"From when Karen Morgan was 12, until she was well into her teens, she was sexually abused by her uncle - a ministerial servant with the Jehovah's Witnesses.</s>\",\n",
       " 'Former Greek Finance Minister Yanis Varoufakis has told the BBC that economic reforms imposed on his country by creditors are \"going to fail\", ahead of talks on a huge bailout.</s><pad>']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(batch['labels'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2022",
   "language": "python",
   "name": "pytorch2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
