{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch benchmark, profiler and JIT compiler\n",
    "\n",
    "In this notebook we will see a simple example on how to use the PyTorch benchmark and profiler, and discuss the impact of Just-in-Time compilation on performance of custom functions.\n",
    "We will use the mean squared error loss for its simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '12' # for Numpy and BLAS\n",
    "  \n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils import data\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import torch.utils.benchmark as benchmark\n",
    "\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample random scalar data\n",
    "nsamples = 20000\n",
    "nfeat = 500\n",
    "\n",
    "x = np.random.random([nsamples, nfeat])\n",
    "y = np.random.random([nsamples, nfeat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(input, target):\n",
    "    '''\n",
    "    measures the mean squared error (squared L2 norm) between\n",
    "    each element in the input `x` and target :`y`\n",
    "    '''\n",
    "    diff = (input - target)  # WARN: this temporary variable makes this function 10-20% slower in Numpy\n",
    "    return (diff**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time the MSE function\n",
    "%timeit -n1 -r100 mse(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch benchmark (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyless Tensors\n",
    "tx = torch.from_numpy(x)\n",
    "ty = torch.from_numpy(y)\n",
    "\n",
    "tx.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(12)\n",
    "\n",
    "%timeit -n1 -r100 mse(tx, ty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "cx = tx.to(device)\n",
    "cy = ty.to(device)\n",
    "\n",
    "# %timeit -n1 -r100 mse(cx, cy) ## !!! Wrong way of time GPU functions, without torch.cuda.synchronize()\n",
    "## https://pytorch.org/tutorials/recipes/recipes/benchmark.html#benchmarking-with-torch-utils-benchmark-timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = benchmark.Timer(\n",
    "    stmt='mse(x, y)',\n",
    "    setup='from __main__ import mse',\n",
    "    globals={'x': cx, 'y': cy})\n",
    "\n",
    "print(t0.timeit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with profile(activities=[ProfilerActivity.CUDA]) as prof:\n",
    "    with record_function(\"mse\"):\n",
    "        mse(cx, cy)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "prof.export_chrome_trace(\"mse.trace.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({e.key:e.__dict__ for e in prof.key_averages()}).T\n",
    "df[['count', 'cpu_time_total', 'cuda_time_total']].sort_values(['cpu_time_total', 'cuda_time_total'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Just-in-Time compiler\n",
    "\n",
    "https://pytorch.org/docs/stable/jit.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scripting vs. Tracing\n",
    "\n",
    "- `torch.jit.script` compiles the function or module into TorchScript, using a \\[large\\] subset of Python\n",
    "- `torch.jit.trace` uses the example input to compute a fixed graph, and therefore cannot handle control flow, e.g. if statements and similar conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jit_mse_scripted = torch.jit.script(mse)\n",
    "print(jit_mse_scripted.code)\n",
    "print(jit_mse_scripted.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jit_mse_traced = torch.jit.trace(mse, example_inputs=(cx, cy))\n",
    "# jit.trace only support Tensors but is ideal for benchmarking\n",
    "print(jit_mse_traced.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def jit_mse(\n",
    "        input,\n",
    "        target,\n",
    "        reduce : bool = True  # types have to be annotated or are assumed Tensors\n",
    "    ):\n",
    "    err = (input - target)**2\n",
    "    return err.mean() if reduce else err\n",
    "\n",
    "print(jit_mse.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = benchmark.Timer(\n",
    "    stmt='jit_mse_traced(x, y)',\n",
    "    setup='from __main__ import jit_mse_traced',\n",
    "    globals={'x': cx, 'y': cy})\n",
    "t1 = benchmark.Timer(\n",
    "    stmt='jit_mse_scripted(x, y)',\n",
    "    setup='from __main__ import jit_mse_scripted',\n",
    "    globals={'x': cx, 'y': cy})\n",
    "t2 = benchmark.Timer(\n",
    "    stmt='jit_mse(x, y)',\n",
    "    setup='from __main__ import jit_mse',\n",
    "    globals={'x': cx, 'y': cy})\n",
    "\n",
    "print(t0.timeit(100))\n",
    "print(t1.timeit(100))\n",
    "print(t2.timeit(100))\n",
    "# torch.jit.script requires a warmup run before benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with profile(activities=[ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    with record_function(\"jit_mse\"):\n",
    "        jit_mse(cx, cy)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "prof.export_chrome_trace(\"jit_mse.trace.json\")\n",
    "\n",
    "df = pd.DataFrame({e.key:e.__dict__ for e in prof.key_averages()}).T\n",
    "df[['count', 'cpu_time_total', 'cuda_time_total']].sort_values(['cpu_time_total', 'cuda_time_total'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing with torch.nn.functional.mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## F.mse_loss(cx, cy)\n",
    "assert jit_mse(cx, cy) - F.mse_loss(cx, cy) < 1e-12\n",
    "\n",
    "t2 = benchmark.Timer(\n",
    "    stmt='mse_loss(x, y)',\n",
    "    setup='from torch.nn.functional import mse_loss',\n",
    "    globals={'x': cx, 'y': cy})\n",
    "\n",
    "print(t2.timeit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with profile(activities=[ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    with record_function(\"mse_loss\"):\n",
    "        F.mse_loss(cx, cy)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "prof.export_chrome_trace(\"mse_loss.trace.json\")\n",
    "\n",
    "df = pd.DataFrame({e.key:e.__dict__ for e in prof.key_averages()}).T\n",
    "df[['count', 'cpu_time_total', 'cuda_time_total']].sort_values(['cpu_time_total', 'cuda_time_total'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our simple JIT'ed function is actually slightly faster than the PyTorch mse_loss with it's custom CUDA kernel, however, the same is not true in backward pass as we can see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = cx.clone()\n",
    "var.requires_grad = True\n",
    "\n",
    "# warm up\n",
    "jit_mse(var, cy).backward()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "t0 = benchmark.Timer(\n",
    "    stmt='jit_mse(x, y).backward()',\n",
    "    setup='from __main__ import jit_mse',\n",
    "    globals={'x': var, 'y': cy})\n",
    "\n",
    "t1 = benchmark.Timer(\n",
    "    stmt='mse_loss(x, y).backward()',\n",
    "    setup='from torch.nn.functional import mse_loss',\n",
    "    globals={'x': var, 'y': cy})\n",
    "\n",
    "print(t0.timeit(1000))\n",
    "print(t1.timeit(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Often JIT'ed autograd functions are faster than autograd of JIT'ed functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def mse_fw(input, target):\n",
    "    err = (input - target)**2\n",
    "    return err.mean()\n",
    "\n",
    "@torch.jit.script\n",
    "def mse_bw(input, target, grad_output):\n",
    "    grad = (input - target) * (grad_output * 2 / input.numel())\n",
    "    return grad, grad\n",
    "\n",
    "\n",
    "class MSE(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, target):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input, target)\n",
    "        return mse_fw(input, target)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, target = ctx.saved_tensors\n",
    "        return mse_bw(input, target, grad_output)\n",
    "\n",
    "\n",
    "# To apply our Function, we use Function.apply method.\n",
    "jit_mse = MSE.apply\n",
    "\n",
    "# some basic testing\n",
    "print(torch.allclose(F.mse_loss(cx, cy), jit_mse(cx, cy)))\n",
    "\n",
    "var.grad = None\n",
    "F.mse_loss(var, cy).backward()\n",
    "fg = var.grad.clone()\n",
    "\n",
    "var.grad = None\n",
    "jit_mse(var, cy).backward()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "print(torch.allclose(fg, var.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = benchmark.Timer(\n",
    "    stmt='jit_mse(x, y).backward()',\n",
    "    setup='from __main__ import jit_mse',\n",
    "    globals={'x': var, 'y': cy})\n",
    "\n",
    "t1 = benchmark.Timer(\n",
    "    stmt='mse_loss(x, y).backward()',\n",
    "    setup='from torch.nn.functional import mse_loss',\n",
    "    globals={'x': var, 'y': cy})\n",
    "\n",
    "print(t0.timeit(100))\n",
    "print(t1.timeit(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TorchScript functions/modules can be easily saved, opened with PyTorch C++ API (LibTorch) and further optimized for inference with NVIDIA TensorRT.**\n",
    "\n",
    "If JIT is not enough for your use case, it is also possible to write your own kernels.\n",
    "1. [PyTorch C++ Extension](https://pytorch.org/tutorials/advanced/cpp_extension.html)\n",
    "2. [OpenAI Triton](https://triton-lang.org/getting-started/tutorials/02-fused-softmax.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepspeed",
   "language": "python",
   "name": "deepspeed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
