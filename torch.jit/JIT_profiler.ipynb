{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch benchmark, profiler and JIT compiler\n",
    "\n",
    "In this notebook we will see a simple example on how to use the PyTorch benchmark and profiler, and discuss the impact of Just-in-Time compilation on performance of custom functions.\n",
    "We will use the mean squared error loss for its simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '12' # for Numpy and BLAS\n",
    "  \n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils import data\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import torch.utils.benchmark as benchmark\n",
    "\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample random scalar data\n",
    "nsamples = 20000\n",
    "nfeat = 500\n",
    "\n",
    "x = np.random.random([nsamples, nfeat])\n",
    "y = np.random.random([nsamples, nfeat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(input, target):\n",
    "    '''\n",
    "    measures the mean squared error (squared L2 norm) between\n",
    "    each element in the input `x` and target :`y`\n",
    "    '''\n",
    "    diff = (input - target)  # WARN: this temporary variable makes this function 10-20% slower in Numpy\n",
    "    return (diff**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.7 ms ± 320 µs per loop (mean ± std. dev. of 100 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Time the MSE function\n",
    "%timeit -n1 -r100 mse(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch benchmark (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copyless Tensors\n",
    "tx = torch.from_numpy(x)\n",
    "ty = torch.from_numpy(y)\n",
    "\n",
    "tx.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.8 ms ± 736 µs per loop (mean ± std. dev. of 100 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "torch.set_num_threads(12)\n",
    "\n",
    "%timeit -n1 -r100 mse(tx, ty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "cx = tx.to(device)\n",
    "cy = ty.to(device)\n",
    "\n",
    "# %timeit -n1 -r100 mse(cx, cy) ## !!! Wrong way of time GPU functions, without torch.cuda.synchronize()\n",
    "## https://pytorch.org/tutorials/recipes/recipes/benchmark.html#benchmarking-with-torch-utils-benchmark-timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x2aabee21ab50>\n",
      "mse(x, y)\n",
      "setup: from __main__ import mse\n",
      "  980.93 us\n",
      "  1 measurement, 100 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "t0 = benchmark.Timer(\n",
    "    stmt='mse(x, y)',\n",
    "    setup='from __main__ import mse',\n",
    "    globals={'x': cx, 'y': cy})\n",
    "\n",
    "print(t0.timeit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     475.000us        48.62%     475.000us     475.000us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     350.000us        35.82%     350.000us     350.000us             1  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     151.000us        15.46%     151.000us     151.000us             1  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us       1.000us         0.10%       1.000us       1.000us             1  \n",
      "                                       cudaLaunchKernel        99.96%        1.571s        99.96%        1.571s     523.677ms       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                        cudaMemsetAsync         0.00%      12.000us         0.00%      12.000us      12.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                  cudaDeviceSynchronize         0.04%     667.000us         0.04%     667.000us     667.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.572s\n",
      "Self CUDA time total: 977.000us\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with profile(activities=[ProfilerActivity.CUDA], record_shapes=True, with_stack=True) as prof:\n",
    "    with record_function(\"mse\"):\n",
    "        mse(cx, cy)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "prof.export_chrome_trace(\"mse.trace.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>cpu_time_total</th>\n",
       "      <th>cuda_time_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cudaLaunchKernel</th>\n",
       "      <td>3</td>\n",
       "      <td>1.57103e+06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudaDeviceSynchronize</th>\n",
       "      <td>1</td>\n",
       "      <td>667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudaMemsetAsync</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>void at::native::vectorized_elementwise_kernel&lt;4, at::native::AddFunctor&lt;double&gt;, at::detail::Array&lt;char*, 3&gt; &gt;(int, at::native::AddFunctor&lt;double&gt;, at::detail::Array&lt;char*, 3&gt;)</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>void at::native::vectorized_elementwise_kernel&lt;4, at::native::(anonymous namespace)::pow_tensor_scalar_kernel_impl&lt;double, double&gt;(at::TensorIteratorBase&amp;, double)::{lambda(double)#1}, at::detail::Array&lt;char*, 2&gt; &gt;(int, at::native::(anonymous namespace)::pow_tensor_scalar_kernel_impl&lt;double, double&gt;(at::TensorIteratorBase&amp;, double)::{lambda(double)#1}, at::detail::Array&lt;char*, 2&gt;)</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>void at::native::reduce_kernel&lt;512, 1, at::native::ReduceOp&lt;double, at::native::MeanOps&lt;double, double&gt;, unsigned int, double, 4&gt; &gt;(at::native::ReduceOp&lt;double, at::native::MeanOps&lt;double, double&gt;, unsigned int, double, 4&gt;)</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Memset (Device)</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   count cpu_time_total  \\\n",
       "cudaLaunchKernel                                       3    1.57103e+06   \n",
       "cudaDeviceSynchronize                                  1            667   \n",
       "cudaMemsetAsync                                        1             12   \n",
       "void at::native::vectorized_elementwise_kernel<...     1              0   \n",
       "void at::native::vectorized_elementwise_kernel<...     1              0   \n",
       "void at::native::reduce_kernel<512, 1, at::nati...     1              0   \n",
       "Memset (Device)                                        1              0   \n",
       "\n",
       "                                                   cuda_time_total  \n",
       "cudaLaunchKernel                                                 0  \n",
       "cudaDeviceSynchronize                                            0  \n",
       "cudaMemsetAsync                                                  0  \n",
       "void at::native::vectorized_elementwise_kernel<...             475  \n",
       "void at::native::vectorized_elementwise_kernel<...             350  \n",
       "void at::native::reduce_kernel<512, 1, at::nati...             151  \n",
       "Memset (Device)                                                  1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({e.key:e.__dict__ for e in prof.key_averages()}).T\n",
    "df[['count', 'cpu_time_total', 'cuda_time_total']].sort_values(['cpu_time_total', 'cuda_time_total'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Just-in-Time compiler\n",
    "\n",
    "https://pytorch.org/docs/stable/jit.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scripting vs. Tracing\n",
    "\n",
    "- `torch.jit.script` compiles the function or module into TorchScript, using a \\[large\\] subset of Python\n",
    "- `torch.jit.trace` uses the example input to compute a fixed graph, and therefore cannot handle control flow, e.g. if statements and similar conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def mse(input: Tensor,\n",
      "    target: Tensor) -> Tensor:\n",
      "  diff = torch.sub(input, target)\n",
      "  return torch.mean(torch.pow(diff, 2))\n",
      "\n",
      "graph(%input.1 : Tensor,\n",
      "      %target.1 : Tensor):\n",
      "  %10 : NoneType = prim::Constant()\n",
      "  %5 : int = prim::Constant[value=1]()\n",
      "  %8 : int = prim::Constant[value=2]() # /tmp/ipykernel_23909/3968975852.py:7:18\n",
      "  %diff.1 : Tensor = aten::sub(%input.1, %target.1, %5) # /tmp/ipykernel_23909/3968975852.py:6:12\n",
      "  %9 : Tensor = aten::pow(%diff.1, %8) # /tmp/ipykernel_23909/3968975852.py:7:12\n",
      "  %11 : Tensor = aten::mean(%9, %10) # /tmp/ipykernel_23909/3968975852.py:7:12\n",
      "  return (%11)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jit_mse_scripted = torch.jit.script(mse)\n",
    "print(jit_mse_scripted.code)\n",
    "print(jit_mse_scripted.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%input : Double(20000, 500, strides=[500, 1], requires_grad=0, device=cuda:0),\n",
      "      %target : Double(20000, 500, strides=[500, 1], requires_grad=0, device=cuda:0)):\n",
      "  %2 : int = prim::Constant[value=1]() # /tmp/ipykernel_23909/3968975852.py:6:0\n",
      "  %diff : Double(20000, 500, strides=[500, 1], requires_grad=0, device=cuda:0) = aten::sub(%input, %target, %2) # /tmp/ipykernel_23909/3968975852.py:6:0\n",
      "  %4 : int = prim::Constant[value=2]() # /tmp/ipykernel_23909/3968975852.py:7:0\n",
      "  %5 : Double(20000, 500, strides=[500, 1], requires_grad=0, device=cuda:0) = aten::pow(%diff, %4) # /tmp/ipykernel_23909/3968975852.py:7:0\n",
      "  %6 : NoneType = prim::Constant()\n",
      "  %7 : Double(requires_grad=0, device=cuda:0) = aten::mean(%5, %6) # /tmp/ipykernel_23909/3968975852.py:7:0\n",
      "  return (%7)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jit_mse_traced = torch.jit.trace(mse, example_inputs=(cx, cy))\n",
    "# jit.trace only support Tensors but is ideal for benchmarking\n",
    "print(jit_mse_traced.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def jit_mse(input: Tensor,\n",
      "    target: Tensor,\n",
      "    reduce: bool=True) -> Tensor:\n",
      "  err = torch.pow(torch.sub(input, target), 2)\n",
      "  if reduce:\n",
      "    _0 = torch.mean(err)\n",
      "  else:\n",
      "    _0 = err\n",
      "  return _0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@torch.jit.script\n",
    "def jit_mse(\n",
    "        input,\n",
    "        target,\n",
    "        reduce : bool = True  # types have to be annotated or are assumed Tensors\n",
    "    ):\n",
    "    err = (input - target)**2\n",
    "    return err.mean() if reduce else err\n",
    "\n",
    "print(jit_mse.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x2aabf4c9d280>\n",
      "jit_mse_traced(x, y)\n",
      "setup: from __main__ import jit_mse_traced\n",
      "  594.18 us\n",
      "  1 measurement, 100 runs , 1 thread\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x2aabf4c9d580>\n",
      "jit_mse_scripted(x, y)\n",
      "setup: from __main__ import jit_mse_scripted\n",
      "  2.06 ms\n",
      "  1 measurement, 100 runs , 1 thread\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x2aabf4c9d640>\n",
      "jit_mse(x, y)\n",
      "setup: from __main__ import jit_mse\n",
      "  2.06 ms\n",
      "  1 measurement, 100 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "t0 = benchmark.Timer(\n",
    "    stmt='jit_mse_traced(x, y)',\n",
    "    setup='from __main__ import jit_mse_traced',\n",
    "    globals={'x': cx, 'y': cy})\n",
    "t1 = benchmark.Timer(\n",
    "    stmt='jit_mse_scripted(x, y)',\n",
    "    setup='from __main__ import jit_mse_scripted',\n",
    "    globals={'x': cx, 'y': cy})\n",
    "t2 = benchmark.Timer(\n",
    "    stmt='jit_mse(x, y)',\n",
    "    setup='from __main__ import jit_mse',\n",
    "    globals={'x': cx, 'y': cy})\n",
    "\n",
    "print(t0.timeit(100))\n",
    "print(t1.timeit(100))\n",
    "print(t2.timeit(100))\n",
    "# torch.jit.script requires a warmup run before benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                          fused_sub_pow         0.00%       0.000us         0.00%       0.000us       0.000us     435.000us        74.11%     435.000us     435.000us             1  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     151.000us        25.72%     151.000us     151.000us             1  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us       1.000us         0.17%       1.000us       1.000us             1  \n",
      "                                        cudaMemsetAsync         3.37%      14.000us         3.37%      14.000us      14.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                       cudaLaunchKernel         3.37%      14.000us         3.37%      14.000us      14.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                  cudaDeviceSynchronize        93.25%     387.000us        93.25%     387.000us     387.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 415.000us\n",
      "Self CUDA time total: 587.000us\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>cpu_time_total</th>\n",
       "      <th>cuda_time_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cudaDeviceSynchronize</th>\n",
       "      <td>1</td>\n",
       "      <td>387</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudaMemsetAsync</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudaLaunchKernel</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fused_sub_pow</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>void at::native::reduce_kernel&lt;512, 1, at::native::ReduceOp&lt;double, at::native::MeanOps&lt;double, double&gt;, unsigned int, double, 4&gt; &gt;(at::native::ReduceOp&lt;double, at::native::MeanOps&lt;double, double&gt;, unsigned int, double, 4&gt;)</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Memset (Device)</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   count cpu_time_total  \\\n",
       "cudaDeviceSynchronize                                  1            387   \n",
       "cudaMemsetAsync                                        1             14   \n",
       "cudaLaunchKernel                                       1             14   \n",
       "fused_sub_pow                                          1              0   \n",
       "void at::native::reduce_kernel<512, 1, at::nati...     1              0   \n",
       "Memset (Device)                                        1              0   \n",
       "\n",
       "                                                   cuda_time_total  \n",
       "cudaDeviceSynchronize                                            0  \n",
       "cudaMemsetAsync                                                  0  \n",
       "cudaLaunchKernel                                                 0  \n",
       "fused_sub_pow                                                  435  \n",
       "void at::native::reduce_kernel<512, 1, at::nati...             151  \n",
       "Memset (Device)                                                  1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with profile(activities=[ProfilerActivity.CUDA], record_shapes=True, with_stack=True) as prof:\n",
    "    with record_function(\"jit_mse\"):\n",
    "        jit_mse(cx, cy)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "prof.export_chrome_trace(\"jit_mse.trace.json\")\n",
    "\n",
    "df = pd.DataFrame({e.key:e.__dict__ for e in prof.key_averages()}).T\n",
    "df[['count', 'cpu_time_total', 'cuda_time_total']].sort_values(['cpu_time_total', 'cuda_time_total'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing with torch.nn.functional.mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x2aabf443c3d0>\n",
      "mse_loss(x, y)\n",
      "setup: from torch.nn.functional import mse_loss\n",
      "  631.63 us\n",
      "  1 measurement, 100 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "## F.mse_loss(cx, cy)\n",
    "assert jit_mse(cx, cy) - F.mse_loss(cx, cy) < 1e-12\n",
    "\n",
    "t2 = benchmark.Timer(\n",
    "    stmt='mse_loss(x, y)',\n",
    "    setup='from torch.nn.functional import mse_loss',\n",
    "    globals={'x': cx, 'y': cy})\n",
    "\n",
    "print(t2.timeit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     474.000us        75.60%     474.000us     474.000us             1  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     152.000us        24.24%     152.000us     152.000us             1  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us       1.000us         0.16%       1.000us       1.000us             1  \n",
      "                                       cudaLaunchKernel        99.96%        1.145s        99.96%        1.145s     572.539ms       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                        cudaMemsetAsync         0.00%      14.000us         0.00%      14.000us      14.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                  cudaDeviceSynchronize         0.04%     437.000us         0.04%     437.000us     437.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.146s\n",
      "Self CUDA time total: 627.000us\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>cpu_time_total</th>\n",
       "      <th>cuda_time_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cudaLaunchKernel</th>\n",
       "      <td>2</td>\n",
       "      <td>1.14508e+06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudaDeviceSynchronize</th>\n",
       "      <td>1</td>\n",
       "      <td>437</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudaMemsetAsync</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>void at::native::vectorized_elementwise_kernel&lt;4, at::native::mse_kernel_cuda(at::TensorIterator&amp;)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(double, double)#1}, at::detail::Array&lt;char*, 3&gt; &gt;(int, at::native::mse_kernel_cuda(at::TensorIterator&amp;)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(double, double)#1}, at::detail::Array&lt;char*, 3&gt;)</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>void at::native::reduce_kernel&lt;512, 1, at::native::ReduceOp&lt;double, at::native::MeanOps&lt;double, double&gt;, unsigned int, double, 4&gt; &gt;(at::native::ReduceOp&lt;double, at::native::MeanOps&lt;double, double&gt;, unsigned int, double, 4&gt;)</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Memset (Device)</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   count cpu_time_total  \\\n",
       "cudaLaunchKernel                                       2    1.14508e+06   \n",
       "cudaDeviceSynchronize                                  1            437   \n",
       "cudaMemsetAsync                                        1             14   \n",
       "void at::native::vectorized_elementwise_kernel<...     1              0   \n",
       "void at::native::reduce_kernel<512, 1, at::nati...     1              0   \n",
       "Memset (Device)                                        1              0   \n",
       "\n",
       "                                                   cuda_time_total  \n",
       "cudaLaunchKernel                                                 0  \n",
       "cudaDeviceSynchronize                                            0  \n",
       "cudaMemsetAsync                                                  0  \n",
       "void at::native::vectorized_elementwise_kernel<...             474  \n",
       "void at::native::reduce_kernel<512, 1, at::nati...             152  \n",
       "Memset (Device)                                                  1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with profile(activities=[ProfilerActivity.CUDA], record_shapes=True, with_stack=True) as prof:\n",
    "    with record_function(\"mse_loss\"):\n",
    "        F.mse_loss(cx, cy)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "prof.export_chrome_trace(\"mse_loss.trace.json\")\n",
    "\n",
    "df = pd.DataFrame({e.key:e.__dict__ for e in prof.key_averages()}).T\n",
    "df[['count', 'cpu_time_total', 'cuda_time_total']].sort_values(['cpu_time_total', 'cuda_time_total'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our simple JIT'ed function is actually slightly faster than the PyTorch mse_loss with it's custom CUDA kernel, however, the same is not true in backward pass as we can see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x2aabf4c9d100>\n",
      "jit_mse(x, y).backward()\n",
      "setup: from __main__ import jit_mse\n",
      "  2.03 ms\n",
      "  1 measurement, 1000 runs , 1 thread\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x2aaab7a33250>\n",
      "mse_loss(x, y).backward()\n",
      "setup: from torch.nn.functional import mse_loss\n",
      "  1.75 ms\n",
      "  1 measurement, 1000 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "var = cx.clone()\n",
    "var.requires_grad = True\n",
    "\n",
    "# warm up\n",
    "jit_mse(var, cy).backward()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "t0 = benchmark.Timer(\n",
    "    stmt='jit_mse(x, y).backward()',\n",
    "    setup='from __main__ import jit_mse',\n",
    "    globals={'x': var, 'y': cy})\n",
    "\n",
    "t1 = benchmark.Timer(\n",
    "    stmt='mse_loss(x, y).backward()',\n",
    "    setup='from torch.nn.functional import mse_loss',\n",
    "    globals={'x': var, 'y': cy})\n",
    "\n",
    "print(t0.timeit(1000))\n",
    "print(t1.timeit(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Often JIT'ed autograd functions are faster than autograd of JIT'ed functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "@torch.jit.script\n",
    "def mse_fw(input, target):\n",
    "    err = (input - target)**2\n",
    "    return err.mean()\n",
    "\n",
    "@torch.jit.script\n",
    "def mse_bw(input, target, grad_output):\n",
    "    grad = (input - target) * (grad_output * 2 / input.numel())\n",
    "    return grad, grad\n",
    "\n",
    "\n",
    "class MSE(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, target):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input, target)\n",
    "        return mse_fw(input, target)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, target = ctx.saved_tensors\n",
    "        return mse_bw(input, target, grad_output)\n",
    "\n",
    "\n",
    "# To apply our Function, we use Function.apply method.\n",
    "jit_mse = MSE.apply\n",
    "\n",
    "# some basic testing\n",
    "print(torch.allclose(F.mse_loss(cx, cy), jit_mse(cx, cy)))\n",
    "\n",
    "var.grad = None\n",
    "F.mse_loss(var, cy).backward()\n",
    "fg = var.grad.clone()\n",
    "\n",
    "var.grad = None\n",
    "jit_mse(var, cy).backward()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "print(torch.allclose(fg, var.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x2aabf4cb6d30>\n",
      "jit_mse(x, y).backward()\n",
      "setup: from __main__ import jit_mse\n",
      "  1.84 ms\n",
      "  1 measurement, 100 runs , 1 thread\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x2aabf4cb6ee0>\n",
      "mse_loss(x, y).backward()\n",
      "setup: from torch.nn.functional import mse_loss\n",
      "  1.75 ms\n",
      "  1 measurement, 100 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "t0 = benchmark.Timer(\n",
    "    stmt='jit_mse(x, y).backward()',\n",
    "    setup='from __main__ import jit_mse',\n",
    "    globals={'x': var, 'y': cy})\n",
    "\n",
    "t1 = benchmark.Timer(\n",
    "    stmt='mse_loss(x, y).backward()',\n",
    "    setup='from torch.nn.functional import mse_loss',\n",
    "    globals={'x': var, 'y': cy})\n",
    "\n",
    "print(t0.timeit(100))\n",
    "print(t1.timeit(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TorchScript functions/modules can be easily saved, opened with PyTorch C++ API (LibTorch) and further optimized for inference with NVIDIA TensorRT.**\n",
    "\n",
    "If JIT is not enough for your use case, it is also possible to write your own kernels.\n",
    "1. [PyTorch C++ Extension](https://pytorch.org/tutorials/advanced/cpp_extension.html)\n",
    "2. [OpenAI Triton](https://triton-lang.org/getting-started/tutorials/02-fused-softmax.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepspeed",
   "language": "python",
   "name": "deepspeed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
