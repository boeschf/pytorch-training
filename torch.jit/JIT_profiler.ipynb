{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch benchmark, profiler and JIT compiler\n",
    "\n",
    "In this notebook we will see a simple example on how to use the PyTorch benchmark and profiler, and discuss the impact of Just-in-Time compilation on performance of custom functions.\n",
    "We will use the mean squared error loss for its simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '12' # for Numpy and BLAS\n",
    "  \n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils import data\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import torch.utils.benchmark as benchmark\n",
    "\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample random scalar data\n",
    "nsamples = 20000\n",
    "nfeat = 500\n",
    "\n",
    "x = np.random.random([nsamples, nfeat])\n",
    "y = np.random.random([nsamples, nfeat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(input, target):\n",
    "    '''\n",
    "    measures the mean squared error (squared L2 norm) between\n",
    "    each element in the input `x` and target :`y`\n",
    "    '''\n",
    "    diff = (input - target)  # WARN: this temporary variable makes this function 10-20% slower in Numpy\n",
    "    return (diff**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.6 ms ± 487 µs per loop (mean ± std. dev. of 100 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Time the MSE function\n",
    "%timeit -n1 -r100 mse(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch benchmark (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copyless Tensors\n",
    "tx = torch.from_numpy(x)\n",
    "ty = torch.from_numpy(y)\n",
    "\n",
    "tx.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.8 ms ± 914 µs per loop (mean ± std. dev. of 100 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "torch.set_num_threads(12)\n",
    "\n",
    "%timeit -n1 -r100 mse(tx, ty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "cx = tx.to(device)\n",
    "cy = ty.to(device)\n",
    "\n",
    "# %timeit -n1 -r100 mse(cx, cy) ## !!! Wrong way of time GPU functions, without torch.cuda.synchronize()\n",
    "## https://pytorch.org/tutorials/recipes/recipes/benchmark.html#benchmarking-with-torch-utils-benchmark-timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x2aabee21a520>\n",
      "mse(x, y)\n",
      "setup: from __main__ import mse\n",
      "  979.80 us\n",
      "  1 measurement, 100 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "t0 = benchmark.Timer(\n",
    "    stmt='mse(x, y)',\n",
    "    setup='from __main__ import mse',\n",
    "    globals={'x': cx, 'y': cy})\n",
    "\n",
    "print(t0.timeit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     474.000us        48.32%     474.000us     474.000us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     354.000us        36.09%     354.000us     354.000us             1  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     152.000us        15.49%     152.000us     152.000us             1  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us       1.000us         0.10%       1.000us       1.000us             1  \n",
      "                                       cudaLaunchKernel        99.96%        1.567s        99.96%        1.567s     522.233ms       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                        cudaMemsetAsync         0.00%      12.000us         0.00%      12.000us      12.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                  cudaDeviceSynchronize         0.04%     666.000us         0.04%     666.000us     666.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.567s\n",
      "Self CUDA time total: 981.000us\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with profile(activities=[ProfilerActivity.CUDA], record_shapes=True, with_stack=True) as prof:\n",
    "    with record_function(\"mse\"):\n",
    "        mse(cx, cy)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "prof.export_chrome_trace(\"mse.trace.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>cpu_time_total</th>\n",
       "      <th>cuda_time_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cudaLaunchKernel</th>\n",
       "      <td>3</td>\n",
       "      <td>1.5667e+06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudaDeviceSynchronize</th>\n",
       "      <td>1</td>\n",
       "      <td>666</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudaMemsetAsync</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>void at::native::vectorized_elementwise_kernel&lt;4, at::native::AddFunctor&lt;double&gt;, at::detail::Array&lt;char*, 3&gt; &gt;(int, at::native::AddFunctor&lt;double&gt;, at::detail::Array&lt;char*, 3&gt;)</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>void at::native::vectorized_elementwise_kernel&lt;4, at::native::(anonymous namespace)::pow_tensor_scalar_kernel_impl&lt;double, double&gt;(at::TensorIteratorBase&amp;, double)::{lambda(double)#1}, at::detail::Array&lt;char*, 2&gt; &gt;(int, at::native::(anonymous namespace)::pow_tensor_scalar_kernel_impl&lt;double, double&gt;(at::TensorIteratorBase&amp;, double)::{lambda(double)#1}, at::detail::Array&lt;char*, 2&gt;)</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>void at::native::reduce_kernel&lt;512, 1, at::native::ReduceOp&lt;double, at::native::MeanOps&lt;double, double&gt;, unsigned int, double, 4&gt; &gt;(at::native::ReduceOp&lt;double, at::native::MeanOps&lt;double, double&gt;, unsigned int, double, 4&gt;)</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Memset (Device)</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   count cpu_time_total  \\\n",
       "cudaLaunchKernel                                       3     1.5667e+06   \n",
       "cudaDeviceSynchronize                                  1            666   \n",
       "cudaMemsetAsync                                        1             12   \n",
       "void at::native::vectorized_elementwise_kernel<...     1              0   \n",
       "void at::native::vectorized_elementwise_kernel<...     1              0   \n",
       "void at::native::reduce_kernel<512, 1, at::nati...     1              0   \n",
       "Memset (Device)                                        1              0   \n",
       "\n",
       "                                                   cuda_time_total  \n",
       "cudaLaunchKernel                                                 0  \n",
       "cudaDeviceSynchronize                                            0  \n",
       "cudaMemsetAsync                                                  0  \n",
       "void at::native::vectorized_elementwise_kernel<...             474  \n",
       "void at::native::vectorized_elementwise_kernel<...             354  \n",
       "void at::native::reduce_kernel<512, 1, at::nati...             152  \n",
       "Memset (Device)                                                  1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({e.key:e.__dict__ for e in prof.key_averages()}).T\n",
    "df[['count', 'cpu_time_total', 'cuda_time_total']].sort_values(['cpu_time_total', 'cuda_time_total'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Just-in-Time compiler\n",
    "\n",
    "https://pytorch.org/docs/stable/jit.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scripting vs. Tracing\n",
    "\n",
    "- `torch.jit.script` compiles the function or module into TorchScript, using a \\[large\\] subset of Python\n",
    "- `torch.jit.trace` uses the example input to compute a fixed graph, and therefore cannot handle control flow, e.g. if statements and similar conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def mse(input: Tensor,\n",
      "    target: Tensor) -> Tensor:\n",
      "  diff = torch.sub(input, target)\n",
      "  return torch.mean(torch.pow(diff, 2))\n",
      "\n",
      "graph(%input.1 : Tensor,\n",
      "      %target.1 : Tensor):\n",
      "  %10 : NoneType = prim::Constant()\n",
      "  %5 : int = prim::Constant[value=1]()\n",
      "  %8 : int = prim::Constant[value=2]() # /tmp/ipykernel_28029/3968975852.py:7:18\n",
      "  %diff.1 : Tensor = aten::sub(%input.1, %target.1, %5) # /tmp/ipykernel_28029/3968975852.py:6:12\n",
      "  %9 : Tensor = aten::pow(%diff.1, %8) # /tmp/ipykernel_28029/3968975852.py:7:12\n",
      "  %11 : Tensor = aten::mean(%9, %10) # /tmp/ipykernel_28029/3968975852.py:7:12\n",
      "  return (%11)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jit_mse_scripted = torch.jit.script(mse)\n",
    "print(jit_mse_scripted.code)\n",
    "print(jit_mse_scripted.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%input : Double(20000, 500, strides=[500, 1], requires_grad=0, device=cuda:0),\n",
      "      %target : Double(20000, 500, strides=[500, 1], requires_grad=0, device=cuda:0)):\n",
      "  %2 : int = prim::Constant[value=1]() # /tmp/ipykernel_28029/3968975852.py:6:0\n",
      "  %diff : Double(20000, 500, strides=[500, 1], requires_grad=0, device=cuda:0) = aten::sub(%input, %target, %2) # /tmp/ipykernel_28029/3968975852.py:6:0\n",
      "  %4 : int = prim::Constant[value=2]() # /tmp/ipykernel_28029/3968975852.py:7:0\n",
      "  %5 : Double(20000, 500, strides=[500, 1], requires_grad=0, device=cuda:0) = aten::pow(%diff, %4) # /tmp/ipykernel_28029/3968975852.py:7:0\n",
      "  %6 : NoneType = prim::Constant()\n",
      "  %7 : Double(requires_grad=0, device=cuda:0) = aten::mean(%5, %6) # /tmp/ipykernel_28029/3968975852.py:7:0\n",
      "  return (%7)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jit_mse_traced = torch.jit.trace(mse, example_inputs=(cx, cy))\n",
    "# jit.trace only support Tensors but is ideal for benchmarking\n",
    "print(jit_mse_traced.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def jit_mse(input: Tensor,\n",
      "    target: Tensor,\n",
      "    reduce: bool=True) -> Tensor:\n",
      "  err = torch.pow(torch.sub(input, target), 2)\n",
      "  if reduce:\n",
      "    _0 = torch.mean(err)\n",
      "  else:\n",
      "    _0 = err\n",
      "  return _0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@torch.jit.script\n",
    "def jit_mse(\n",
    "        input,\n",
    "        target,\n",
    "        reduce : bool = True  # types have to be annotated or are assumed Tensors\n",
    "    ):\n",
    "    err = (input - target)**2\n",
    "    return err.mean() if reduce else err\n",
    "\n",
    "print(jit_mse.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x2aabf4ca3550>\n",
      "jit_mse_traced(x, y)\n",
      "setup: from __main__ import jit_mse_traced\n",
      "  594.09 us\n",
      "  1 measurement, 100 runs , 1 thread\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x2aabf4ca3850>\n",
      "jit_mse_scripted(x, y)\n",
      "setup: from __main__ import jit_mse_scripted\n",
      "  2.06 ms\n",
      "  1 measurement, 100 runs , 1 thread\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x2aabf4ca3910>\n",
      "jit_mse(x, y)\n",
      "setup: from __main__ import jit_mse\n",
      "  2.06 ms\n",
      "  1 measurement, 100 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "t0 = benchmark.Timer(\n",
    "    stmt='jit_mse_traced(x, y)',\n",
    "    setup='from __main__ import jit_mse_traced',\n",
    "    globals={'x': cx, 'y': cy})\n",
    "t1 = benchmark.Timer(\n",
    "    stmt='jit_mse_scripted(x, y)',\n",
    "    setup='from __main__ import jit_mse_scripted',\n",
    "    globals={'x': cx, 'y': cy})\n",
    "t2 = benchmark.Timer(\n",
    "    stmt='jit_mse(x, y)',\n",
    "    setup='from __main__ import jit_mse',\n",
    "    globals={'x': cx, 'y': cy})\n",
    "\n",
    "print(t0.timeit(100))\n",
    "print(t1.timeit(100))\n",
    "print(t2.timeit(100))\n",
    "# torch.jit.script requires a warmup run before benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                          fused_sub_pow         0.00%       0.000us         0.00%       0.000us       0.000us     435.000us        74.11%     435.000us     435.000us             1  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     151.000us        25.72%     151.000us     151.000us             1  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us       1.000us         0.17%       1.000us       1.000us             1  \n",
      "                                        cudaMemsetAsync         3.89%      16.000us         3.89%      16.000us      16.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                       cudaLaunchKernel         3.41%      14.000us         3.41%      14.000us      14.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                  cudaDeviceSynchronize        92.70%     381.000us        92.70%     381.000us     381.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 411.000us\n",
      "Self CUDA time total: 587.000us\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>cpu_time_total</th>\n",
       "      <th>cuda_time_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cudaDeviceSynchronize</th>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudaMemsetAsync</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudaLaunchKernel</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fused_sub_pow</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>void at::native::reduce_kernel&lt;512, 1, at::native::ReduceOp&lt;double, at::native::MeanOps&lt;double, double&gt;, unsigned int, double, 4&gt; &gt;(at::native::ReduceOp&lt;double, at::native::MeanOps&lt;double, double&gt;, unsigned int, double, 4&gt;)</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Memset (Device)</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   count cpu_time_total  \\\n",
       "cudaDeviceSynchronize                                  1            381   \n",
       "cudaMemsetAsync                                        1             16   \n",
       "cudaLaunchKernel                                       1             14   \n",
       "fused_sub_pow                                          1              0   \n",
       "void at::native::reduce_kernel<512, 1, at::nati...     1              0   \n",
       "Memset (Device)                                        1              0   \n",
       "\n",
       "                                                   cuda_time_total  \n",
       "cudaDeviceSynchronize                                            0  \n",
       "cudaMemsetAsync                                                  0  \n",
       "cudaLaunchKernel                                                 0  \n",
       "fused_sub_pow                                                  435  \n",
       "void at::native::reduce_kernel<512, 1, at::nati...             151  \n",
       "Memset (Device)                                                  1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with profile(activities=[ProfilerActivity.CUDA], record_shapes=True, with_stack=True) as prof:\n",
    "    with record_function(\"jit_mse\"):\n",
    "        jit_mse(cx, cy)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "prof.export_chrome_trace(\"jit_mse.trace.json\")\n",
    "\n",
    "df = pd.DataFrame({e.key:e.__dict__ for e in prof.key_averages()}).T\n",
    "df[['count', 'cpu_time_total', 'cuda_time_total']].sort_values(['cpu_time_total', 'cuda_time_total'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing with torch.nn.functional.mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x2aabf443d0d0>\n",
      "mse_loss(x, y)\n",
      "setup: from torch.nn.functional import mse_loss\n",
      "  631.80 us\n",
      "  1 measurement, 100 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "## F.mse_loss(cx, cy)\n",
    "assert jit_mse(cx, cy) - F.mse_loss(cx, cy) < 1e-12\n",
    "\n",
    "t2 = benchmark.Timer(\n",
    "    stmt='mse_loss(x, y)',\n",
    "    setup='from torch.nn.functional import mse_loss',\n",
    "    globals={'x': cx, 'y': cy})\n",
    "\n",
    "print(t2.timeit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     473.000us        75.68%     473.000us     473.000us             1  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     151.000us        24.16%     151.000us     151.000us             1  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us       1.000us         0.16%       1.000us       1.000us             1  \n",
      "                                       cudaLaunchKernel        99.96%        1.144s        99.96%        1.144s     572.192ms       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                        cudaMemsetAsync         0.00%      15.000us         0.00%      15.000us      15.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                  cudaDeviceSynchronize         0.04%     428.000us         0.04%     428.000us     428.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.145s\n",
      "Self CUDA time total: 625.000us\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>cpu_time_total</th>\n",
       "      <th>cuda_time_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cudaLaunchKernel</th>\n",
       "      <td>2</td>\n",
       "      <td>1.14438e+06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudaDeviceSynchronize</th>\n",
       "      <td>1</td>\n",
       "      <td>428</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudaMemsetAsync</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>void at::native::vectorized_elementwise_kernel&lt;4, at::native::mse_kernel_cuda(at::TensorIterator&amp;)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(double, double)#1}, at::detail::Array&lt;char*, 3&gt; &gt;(int, at::native::mse_kernel_cuda(at::TensorIterator&amp;)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(double, double)#1}, at::detail::Array&lt;char*, 3&gt;)</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>void at::native::reduce_kernel&lt;512, 1, at::native::ReduceOp&lt;double, at::native::MeanOps&lt;double, double&gt;, unsigned int, double, 4&gt; &gt;(at::native::ReduceOp&lt;double, at::native::MeanOps&lt;double, double&gt;, unsigned int, double, 4&gt;)</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Memset (Device)</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   count cpu_time_total  \\\n",
       "cudaLaunchKernel                                       2    1.14438e+06   \n",
       "cudaDeviceSynchronize                                  1            428   \n",
       "cudaMemsetAsync                                        1             15   \n",
       "void at::native::vectorized_elementwise_kernel<...     1              0   \n",
       "void at::native::reduce_kernel<512, 1, at::nati...     1              0   \n",
       "Memset (Device)                                        1              0   \n",
       "\n",
       "                                                   cuda_time_total  \n",
       "cudaLaunchKernel                                                 0  \n",
       "cudaDeviceSynchronize                                            0  \n",
       "cudaMemsetAsync                                                  0  \n",
       "void at::native::vectorized_elementwise_kernel<...             473  \n",
       "void at::native::reduce_kernel<512, 1, at::nati...             151  \n",
       "Memset (Device)                                                  1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with profile(activities=[ProfilerActivity.CUDA], record_shapes=True, with_stack=True) as prof:\n",
    "    with record_function(\"mse_loss\"):\n",
    "        F.mse_loss(cx, cy)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "prof.export_chrome_trace(\"mse_loss.trace.json\")\n",
    "\n",
    "df = pd.DataFrame({e.key:e.__dict__ for e in prof.key_averages()}).T\n",
    "df[['count', 'cpu_time_total', 'cuda_time_total']].sort_values(['cpu_time_total', 'cuda_time_total'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our simple JIT'ed function is actually slightly faster than the PyTorch mse_loss with it's custom CUDA kernel, however, the same is not true in backward pass as we can see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x2aabf4ca3640>\n",
      "jit_mse(x, y).backward()\n",
      "setup: from __main__ import jit_mse\n",
      "  2.04 ms\n",
      "  1 measurement, 1000 runs , 1 thread\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x2aabf4ca3400>\n",
      "mse_loss(x, y).backward()\n",
      "setup: from torch.nn.functional import mse_loss\n",
      "  1.75 ms\n",
      "  1 measurement, 1000 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "var = cx.clone()\n",
    "var.requires_grad = True\n",
    "\n",
    "# warm up\n",
    "jit_mse(var, cy).backward()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "t0 = benchmark.Timer(\n",
    "    stmt='jit_mse(x, y).backward()',\n",
    "    setup='from __main__ import jit_mse',\n",
    "    globals={'x': var, 'y': cy})\n",
    "\n",
    "t1 = benchmark.Timer(\n",
    "    stmt='mse_loss(x, y).backward()',\n",
    "    setup='from torch.nn.functional import mse_loss',\n",
    "    globals={'x': var, 'y': cy})\n",
    "\n",
    "print(t0.timeit(1000))\n",
    "print(t1.timeit(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Often JIT'ed autograd functions are faster than autograd of JIT'ed functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def mse_fw(input, target):\n",
    "    err = (input - target)**2\n",
    "    return err.mean()\n",
    "\n",
    "@torch.jit.script\n",
    "def mse_bw(input, target, grad_output):\n",
    "    grad = (input - target) * (2 / input.numel()) * grad_output\n",
    "    return grad, grad\n",
    "\n",
    "\n",
    "class MSE(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, target):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input, target)\n",
    "        return mse_fw(input, target)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, target = ctx.saved_tensors\n",
    "        return mse_bw(input, target, grad_output)\n",
    "\n",
    "\n",
    "# To apply our Function, we use Function.apply method.\n",
    "jit_mse = MSE.apply\n",
    "\n",
    "# some basic testing\n",
    "assert torch.allclose(F.mse_loss(cx, cy), jit_mse(cx, cy))\n",
    "\n",
    "var.grad = None\n",
    "F.mse_loss(var, cy).backward()\n",
    "fg = var.grad.clone()\n",
    "\n",
    "var.grad = None\n",
    "jit_mse(var, cy).backward()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "assert torch.allclose(fg, var.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x2aabf4cb14c0>\n",
      "jit_mse(x, y).backward()\n",
      "setup: from __main__ import jit_mse\n",
      "  1.80 ms\n",
      "  1 measurement, 100 runs , 1 thread\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x2aabf4cb1670>\n",
      "mse_loss(x, y).backward()\n",
      "setup: from torch.nn.functional import mse_loss\n",
      "  1.75 ms\n",
      "  1 measurement, 100 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "t0 = benchmark.Timer(\n",
    "    stmt='jit_mse(x, y).backward()',\n",
    "    setup='from __main__ import jit_mse',\n",
    "    globals={'x': var, 'y': cy})\n",
    "\n",
    "t1 = benchmark.Timer(\n",
    "    stmt='mse_loss(x, y).backward()',\n",
    "    setup='from torch.nn.functional import mse_loss',\n",
    "    globals={'x': var, 'y': cy})\n",
    "\n",
    "print(t0.timeit(100))\n",
    "print(t1.timeit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>cpu_time_total</th>\n",
       "      <th>cuda_time_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>torch::autograd::AccumulateGrad</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aten::add_</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>void at::native::vectorized_elementwise_kernel&lt;4, at::native::AddFunctor&lt;double&gt;, at::detail::Array&lt;char*, 3&gt; &gt;(int, at::native::AddFunctor&lt;double&gt;, at::detail::Array&lt;char*, 3&gt;)</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fused_sub_pow</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fused_sub_mul</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSEBackward</th>\n",
       "      <td>1</td>\n",
       "      <td>202</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mse_bw</th>\n",
       "      <td>1</td>\n",
       "      <td>134</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aten::mul</th>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>void at::native::unrolled_elementwise_kernel&lt;at::native::MulFunctor&lt;double&gt;, at::detail::Array&lt;char*, 3&gt;, OffsetCalculator&lt;2, unsigned int&gt;, OffsetCalculator&lt;1, unsigned int&gt;, at::native::memory::LoadWithoutCast, at::native::memory::StoreWithoutCast&gt;(int, at::native::MulFunctor&lt;double&gt;, at::detail::Array&lt;char*, 3&gt;, OffsetCalculator&lt;2, unsigned int&gt;, OffsetCalculator&lt;1, unsigned int&gt;, at::native::memory::LoadWithoutCast, at::native::memory::StoreWithoutCast)</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jit_mse_backward</th>\n",
       "      <td>1</td>\n",
       "      <td>1.13924e+06</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE</th>\n",
       "      <td>1</td>\n",
       "      <td>1.13848e+06</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mse_fw</th>\n",
       "      <td>1</td>\n",
       "      <td>1.13839e+06</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aten::mean</th>\n",
       "      <td>1</td>\n",
       "      <td>159</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>void at::native::reduce_kernel&lt;512, 1, at::native::ReduceOp&lt;double, at::native::MeanOps&lt;double, double&gt;, unsigned int, double, 4&gt; &gt;(at::native::ReduceOp&lt;double, at::native::MeanOps&lt;double, double&gt;, unsigned int, double, 4&gt;)</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aten::ones_like</th>\n",
       "      <td>1</td>\n",
       "      <td>216</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aten::fill_</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>void at::native::vectorized_elementwise_kernel&lt;4, at::native::FillFunctor&lt;double&gt;, at::detail::Array&lt;char*, 1&gt; &gt;(int, at::native::FillFunctor&lt;double&gt;, at::detail::Array&lt;char*, 1&gt;)</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Memset (Device)</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorExpr</th>\n",
       "      <td>2</td>\n",
       "      <td>1.1382e+06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudaDeviceSynchronize</th>\n",
       "      <td>1</td>\n",
       "      <td>911</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aten::zeros</th>\n",
       "      <td>1</td>\n",
       "      <td>241</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aten::empty_like</th>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aten::empty</th>\n",
       "      <td>3</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudaLaunchKernel</th>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aten::empty_strided</th>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aten::zero_</th>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cudaMemsetAsync</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aten::as_strided</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   count cpu_time_total  \\\n",
       "torch::autograd::AccumulateGrad                        1             46   \n",
       "aten::add_                                             1             32   \n",
       "void at::native::vectorized_elementwise_kernel<...     1              0   \n",
       "fused_sub_pow                                          1              0   \n",
       "fused_sub_mul                                          1              0   \n",
       "MSEBackward                                            1            202   \n",
       "mse_bw                                                 1            134   \n",
       "aten::mul                                              1             44   \n",
       "void at::native::unrolled_elementwise_kernel<at...     1              0   \n",
       "jit_mse_backward                                       1    1.13924e+06   \n",
       "MSE                                                    1    1.13848e+06   \n",
       "mse_fw                                                 1    1.13839e+06   \n",
       "aten::mean                                             1            159   \n",
       "void at::native::reduce_kernel<512, 1, at::nati...     1              0   \n",
       "aten::ones_like                                        1            216   \n",
       "aten::fill_                                            1             60   \n",
       "void at::native::vectorized_elementwise_kernel<...     1              0   \n",
       "Memset (Device)                                        1              0   \n",
       "TensorExpr                                             2     1.1382e+06   \n",
       "cudaDeviceSynchronize                                  1            911   \n",
       "aten::zeros                                            1            241   \n",
       "aten::empty_like                                       1             82   \n",
       "aten::empty                                            3             81   \n",
       "cudaLaunchKernel                                       4             60   \n",
       "aten::empty_strided                                    1             43   \n",
       "aten::zero_                                            1             27   \n",
       "cudaMemsetAsync                                        1             17   \n",
       "aten::as_strided                                       1              7   \n",
       "\n",
       "                                                   cuda_time_total  \n",
       "torch::autograd::AccumulateGrad                                471  \n",
       "aten::add_                                                     471  \n",
       "void at::native::vectorized_elementwise_kernel<...             471  \n",
       "fused_sub_pow                                                  436  \n",
       "fused_sub_mul                                                  431  \n",
       "MSEBackward                                                    301  \n",
       "mse_bw                                                         301  \n",
       "aten::mul                                                      301  \n",
       "void at::native::unrolled_elementwise_kernel<at...             301  \n",
       "jit_mse_backward                                               154  \n",
       "MSE                                                            152  \n",
       "mse_fw                                                         152  \n",
       "aten::mean                                                     152  \n",
       "void at::native::reduce_kernel<512, 1, at::nati...             151  \n",
       "aten::ones_like                                                  2  \n",
       "aten::fill_                                                      2  \n",
       "void at::native::vectorized_elementwise_kernel<...               2  \n",
       "Memset (Device)                                                  1  \n",
       "TensorExpr                                                       0  \n",
       "cudaDeviceSynchronize                                            0  \n",
       "aten::zeros                                                      0  \n",
       "aten::empty_like                                                 0  \n",
       "aten::empty                                                      0  \n",
       "cudaLaunchKernel                                                 0  \n",
       "aten::empty_strided                                              0  \n",
       "aten::zero_                                                      0  \n",
       "cudaMemsetAsync                                                  0  \n",
       "aten::as_strided                                                 0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True, with_stack=True) as prof:\n",
    "    with record_function(\"jit_mse_backward\"):\n",
    "        jit_mse(var, cy).backward()\n",
    "\n",
    "prof.export_chrome_trace(\"jit_mse_backward.trace.json\")\n",
    "\n",
    "df = pd.DataFrame({e.key:e.__dict__ for e in prof.key_averages()}).T\n",
    "df[['count', 'cpu_time_total', 'cuda_time_total']].sort_values(['cuda_time_total', 'cpu_time_total'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**TorchScript functions/modules can be easily saved, opened with PyTorch C++ API (LibTorch) and further optimized for inference with NVIDIA TensorRT.**\n",
    "\n",
    "If JIT is not enough for your use case, it is also possible to write your own kernels.\n",
    "1. [PyTorch C++ Extension](https://pytorch.org/tutorials/advanced/cpp_extension.html)\n",
    "2. [OpenAI Triton](https://triton-lang.org/getting-started/tutorials/02-fused-softmax.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepspeed",
   "language": "python",
   "name": "deepspeed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
