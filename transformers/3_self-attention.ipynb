{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "380478ca-79e3-49a0-8676-ee8cf2dea112",
   "metadata": {},
   "source": [
    "# BERT's Anatomy Step by Step: Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef9a343-af66-40c2-bbb2-ae0473c41e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from transformers import BertForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cd3506-1e3c-4cf0-8663-e000df0a6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'bert-base-uncased'\n",
    "\n",
    "model = BertForPreTraining.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "config = AutoConfig.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87b5c19-faa4-47e5-bb34-f4d607ba26fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(\"let's tokenize something?\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee421f5c-0ea6-4f77-b15d-82a075a1f9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(encoding.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e4ef01-b588-4750-b3a7-82cb25f296b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_embedding = model.bert.embeddings.word_embeddings(encoding)\n",
    "seq_embedding.shape   # (batch_size, seq_len, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9543df31-2be6-458b-867d-cb8827259911",
   "metadata": {},
   "source": [
    "## Self Attention\n",
    "\n",
    "In the context of deep learning, attention is a mechanism that enables a model to focus on specific parts of input data while processing information. It allows the model to assign varying degrees of importance or relevance to different elements of the input, rather than treating all elements uniformly.\n",
    "\n",
    "Attention mechanisms have been widely employed in natural language processing tasks, such as machine translation, text summarization, and sentiment analysis, as well as in computer vision applications, allowing models to selectively attend to relevant regions of an image or sequence. The concept is inspired by human cognitive processes that involve selectively focusing on specific information to better comprehend and process complex data.\n",
    "\n",
    "### A basic self attention implementation\n",
    "\n",
    "Let's consider the following sequence\n",
    "$$\n",
    "\\text{seq} = [~~\\text{`this'}, ~~\\text{`is'}, ~~\\text{`a'}, ~~\\text{`sequence'}~~]\n",
    "$$\n",
    "\n",
    "Replacing the tokens by the embedding vectors, we can see it as a `(seq_len, hidden_size)` matrix $E$.\n",
    "$$\n",
    "\\text{seq} \\equiv E = [~~~~~\\vec{e}_1, ~~~~~~~~\\vec{e}_2, ~~~~~~~~\\vec{e}_3, ~~~~~~~\\vec{e}_4~~~~]\n",
    "$$ \n",
    "\n",
    "The relationship between token $i$ and token $j$ within the sentence can be determined by calculating the attention of token $i$ towards token $j$.\n",
    "For that we can use cosine similarity betwen them.\n",
    "To make it simpler, let's write it only as the dot product between the vectors and leave out the normalization.\n",
    "$$\n",
    "a_{ij} = \\vec{e}_{i} \\cdot \\vec{e}_{j}\n",
    "$$\n",
    "\n",
    "Considering all embeding vectors together as the matrix $E$, we can compute the attentions all at once by performing the matrix multiplication $EE^{\\text{T}}$. That's `(seq_len, hidden_size)` x `(hidden_size, seq_len)` that gives `(seq_len, seq_len)`. Elementwise, we have\n",
    "$$\n",
    "a_{ij} = \\sum_k^{\\mathrm{hid\\ size}} e_{ik} e_{kj}\n",
    "$$\n",
    "where $e_{ik}$ are the elements of the matrix $E$ and $a_{ij}$, the elements of the attention matrix.\n",
    "\n",
    "We could normalize the $a_{ij}$ with a softmax to ensure all the columns or rows sum to 1, but that's not necessary since at this point we only want to understand the concept..\n",
    "\n",
    "Finally, each of the embedding vectors is written as an average of all the embedding vectors weighted by the correspoding attention coefficients\n",
    "$$\n",
    "\\text{seq}^* \\equiv \\text{E}^*_{ij} = \\sum_k^{\\mathrm{hid\\ size}} a_{ik}e_{kj}\n",
    "$$\n",
    "\n",
    "That's the matrix multiplication $AE^{\\text{T}}$, where the shapes are `(seq_len, seq_len)` x `(seq_len, hidden_size)`  -> `(seq_len, hidden_size)`.\n",
    "\n",
    "If the averaging step was not clear, prehaps it's easier if written explicitly for our example `\"This is a sequence\"`:\n",
    "$$\n",
    "\\vec{e}^*_{i} = a_{i1}\\vec{e}_{1} + a_{i2}\\vec{e}_{2} + a_{i3}\\vec{e}_{3} + a_{i4}\\vec{e}_{4}\n",
    "$$\n",
    "The original embedding vector representation of each token has been modified by adding information of the context in which the token is given. That's called **contextual representation**. For instance, now the word \"flies\" is represented differently in the famous examples \"time flies like an arrow\" and \"fruit flies like bananas\".\n",
    "\n",
    "### Scaled dot product attention\n",
    "\n",
    "> from Natural Language Processing with Transformers, Revised Edition\n",
    "\n",
    "The basic attention mechanism above will assign a very large score to identical words in the context, and in particular to the current word itself: the dot product of a query with itself is 1. But in practice, the meaning of a word will be better informed by complementary words in the context than by identical words—for example, the meaning of “flies” is better defined by incorporating information from “time” and “arrow” than by another mention of “flies”.\n",
    "Let’s allow the model to create a different set of vectors for the query, key, and value of a token by using three different linear projections to project our initial token vector into three different spaces.\n",
    "\n",
    "$$\n",
    "Q = W^{\\text{q}}E\n",
    "$$\n",
    "$$\n",
    "K = W^{\\text{k}}E\n",
    "$$\n",
    "$$\n",
    "V = W^{\\text{v}}E\n",
    "$$\n",
    "\n",
    "Now the first matrix multiplication $EE^{\\text{T}}$ that we did to get the similarities becomes $EQ^{\\text{T}}$:\n",
    "$$\n",
    "\\color{gray} EE^{\\text{T}}\n",
    " ~~~ \\Rightarrow  ~~~\n",
    "\\color{black} QK^{\\text{T}}\n",
    "$$\n",
    "Then that's notmilzed by the square root of the size of the matrix $d_k$ and a softmax function is applied to it.\n",
    "Now the attention coefficients are\n",
    "$$\n",
    "a_{ij} = \\sum_k^{\\mathrm{hid\\ size}} e_{ik} e_{kj}\n",
    "~~~ \\Rightarrow  ~~~\n",
    "a_{ij} = \\text{softmax} \\left( \\frac{Q K^{\\text{T}}}{\\sqrt{d_k}} \\right)_{ij}\n",
    "% a = \\text{softmax} \\left( \\frac{\\mathbf{q} \\cdot \\mathbf{k}}{\\sqrt{d_k}} \\right)\n",
    "$$\n",
    "\n",
    "In the scaled dot product attention, the $a_{ij}$ are normalized with a softmax to ensure all the columns or rows sum to 1.\n",
    "\n",
    "$$\n",
    "\\text{seq}^* \\equiv \\text{E}^*_{ij} = \\sum_k^{\\mathrm{hid\\ size}} a_{ik}e_{kj}\n",
    "~~~ \\Rightarrow  ~~~\n",
    "\\text{seq}^* \\equiv \\text{E}^*_{ij} = \\sum_k^{\\mathrm{hid\\ size}} a_{ik}v_{kj}\n",
    "$$\n",
    "\n",
    "We often see the whole thing written in one step as\n",
    "$$\n",
    "\\text{E}^* = \\text{softmax} \\left( \\frac{Q K^{\\text{T}}}{\\sqrt{d_k}} \\right)V\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf23270-fcdf-4c79-8d6c-9699934a26e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_dim = config.hidden_size // config.num_attention_heads\n",
    "\n",
    "query = nn.Linear(config.hidden_size, 64)(seq_embedding)  # Q = W_qE\n",
    "key = nn.Linear(config.hidden_size, 64)(seq_embedding)    # K = W_kE\n",
    "value = nn.Linear(config.hidden_size, 64)(seq_embedding)  # V = W_vE\n",
    "\n",
    "dim_k = query.shape[-1]\n",
    "\n",
    "scores = torch.bmm(query, key.transpose(1, 2)) / torch.math.sqrt(dim_k)  # QK^T / sqrt(dim_k)\n",
    "\n",
    "weights = F.softmax(scores, dim=-1)  # softmax(QK^T / sqrt(dim_k))\n",
    "\n",
    "seq_embedding_att = torch.bmm(weights, value)   # softmax(QK^T / sqrt(dim_k))V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c3808e-eb8f-489b-9872-b564f901ff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(weights.detach().numpy()[0], cmap='Blues', interpolation='nearest')\n",
    "plt.xticks(range(weights.shape[-1]), tokens, rotation=80)\n",
    "plt.yticks(range(weights.shape[-1]), tokens)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.grid(color='w', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2988d68-3fd5-41bf-8078-5f21328e96e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(seq_embedding):\n",
    "    head_dim = config.hidden_size // config.num_attention_heads\n",
    "    query = nn.Linear(config.hidden_size, head_dim)(seq_embedding)\n",
    "    key = nn.Linear(config.hidden_size, head_dim)(seq_embedding)\n",
    "    value = nn.Linear(config.hidden_size, head_dim)(seq_embedding)\n",
    "    dim_k = query.size(-1)\n",
    "    scores = torch.bmm(query, key.transpose(1, 2)) / torch.math.sqrt(dim_k)\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return torch.bmm(weights, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fa261f-afc9-4e76-a7cb-070f4f886d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_concat = torch.cat([scaled_dot_product_attention(seq_embedding) for i in range(12)], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aab79f-9452-4ff6-808f-0a6c4f101b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Linear(config.hidden_size, config.hidden_size)(att_concat).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336c2dd5-a603-479c-b413-a851ff62dc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_concat.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2024",
   "language": "python",
   "name": "ml2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
