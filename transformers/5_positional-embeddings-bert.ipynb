{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "380478ca-79e3-49a0-8676-ee8cf2dea112",
   "metadata": {},
   "source": [
    "# BERT's Anatomy Step by Step: Positional Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef9a343-af66-40c2-bbb2-ae0473c41e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from transformers import BertForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cd3506-1e3c-4cf0-8663-e000df0a6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6d8daa-9f35-485e-b07b-c8e62060640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForPreTraining.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "config = AutoConfig.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87b5c19-faa4-47e5-bb34-f4d607ba26fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(\"let's tokenize something?\", return_tensors=\"pt\")\n",
    "# tokens = tokenizer.convert_ids_to_tokens(encoding.flatten())\n",
    "seq_embedding = model.bert.embeddings.word_embeddings(encoding)\n",
    "seq_embedding.shape   # (batch_size, seq_len, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6335eded-6900-447f-a760-c0693689b37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.bert.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7eb092-f6c8-45f1-b960-bf49bb7fad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.hidden_size  # size of the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa73fad3-eb6c-45c3-8efc-278e8ecd4af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.max_position_embeddings  # max seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4e34ae-5549-4e59-bc7b-e28822b8e3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = encoding.shape[-1]\n",
    "\n",
    "positions = torch.arange(0, seq_len)\n",
    "positions = positions.reshape((1, seq_len))   # make it (batch_size, seq_len)\n",
    "positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d700f10c-7997-4df2-9a7f-bbfcb60f6bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embedding_511 = model.bert.embeddings.position_embeddings(positions)\n",
    "pos_embedding_511.shape  # (batch_size, seq_len, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb3c808-bc75-417b-adc0-b63d1da9404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_embedding + pos_embedding_511"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052b9cdf-b3d6-44fd-8031-a1fb2d2c7498",
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = torch.arange(0, config.max_position_embeddings)\n",
    "# positions = positions.reshape((1, config.max_position_embeddings))    # make it (batch_size, seq_len)\n",
    "\n",
    "pos_embedding = model.bert.embeddings.position_embeddings(positions)\n",
    "pos_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99022d60-d942-495c-adbd-c3f9af200148",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (12, 1)\n",
    "\n",
    "for i in [0, 1, 2, 10, 100, 200, 300, 400, 500]:\n",
    "    # plt.plot((seq_embedding[0, 2] + pos_embedding[i]).detach().numpy(), c='green')\n",
    "    plt.plot(pos_embedding.detach().numpy()[i],    alpha=0.5, c='red')\n",
    "    plt.plot(seq_embedding[0, 2].detach().numpy(), alpha=0.5, c='blue')\n",
    "    plt.xlim([0, config.hidden_size])\n",
    "    plt.ylim([-0.15, 0.15])\n",
    "    plt.show()\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (6, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cc2d42-549e-48cb-852f-42ad56d14391",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = cosine_similarity(pos_embedding.detach().numpy())\n",
    "plt.imshow(similarity_matrix, cmap='Blues')  #, aspect='auto', extent=[0, max_len, 0, max_len])\n",
    "# plt.colorbar()\n",
    "# plt.title('Position-wise Similarity of Positional Embeddings')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Position')\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2024",
   "language": "python",
   "name": "ml2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
